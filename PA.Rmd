---
title: "Robust Methods of Portfolio Optimization Exemplified by the Swiss Market Index\\vspace{0.1cm}"
subtitle: "School of Engineering, Zurich University of Applied Sciences"
author:
  - Pascal Aigner
  - Maurice Gerber
  - Basil Rohr
date: "`r format(Sys.time(), '%d. %B %Y')`"
abstract: "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\\par\\textbf{Keywords:} Portfolio optimization, Robust optimization, Markowitz model, Covariance matrix, Shrinkage, Sharpe ratio\\newpage"
output:
  bookdown::pdf_book:
    toc: F
classoption: twocolumn
geometry: margin = 20mm
documentclass: extarticle
fontsize: 8pt
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \floatplacement{table}{H}
  - \setlength{\abovecaptionskip}{0pt}
---

```{r, include = F}
knitr::opts_chunk$set(echo = F)
library(knitr)
library(ggplot2)
library(ggcorrplot)
library(cowplot)
library(ggrepel)
library(dplyr)
library(grid)

load("./Data/data_1d.Rda")
load("./Data/Rf_1d.Rda")
load("./Data/returns_1d.Rda")
R.utils::sourceDirectory("./Code", modifiedOnly = F)
```

# Introduction
Markowitz' Optimization and other models of Finance require estimates of the expected return, the standard deviation and the correlations of individual assets.
These estimates are subject to estimation errors, especially when they are based on short historical time series. This leads to the danger of over- fitting historical data and - in the case of portfolio optimization - of producing portfolios that perform well in backtesting, but perform poorly in the future.
To overcome these problems, several approaches have been proposed. This includes Bayesian approaches, including shrinking Estimators for historical returns, volatilites and correlations. It also includes resampling/bootstrapping approaches, where expected returns, volatilites and correlations are modelled as random variables.
We will analyze a variety of such approaches, identify those that add most value in the context of investment management, and combine/improve them. The goal is a recommended methodology for computing the strategic asset allocation of pension funds and other institutional investors.

Lange, min. eine halbe Seite

\newpage

# Data
For this paper historical data from the Swiss Market Index or (SMI) were used, the data are from the period from `r format(df[,1][1], "%d.%m.%Y")` to `r format(df[,1][nrow(df)], "%d.%m.%Y")`. The SMI is the most important stock index in Switzerland and contains the 20 largest companies traded on the Swiss stock exchange. The SMI covers approximately 80% of the total capitalization of the Swiss stock market. It is also a price index, which means that dividends are not included in the index. The SMI is always reviewed twice a year and, if necessary, reassembled. In our case, we use the composition per end of 2018. The reason for the 2018 composition is, that enough historical data for all those companies is available. The results shown in table \@ref(tab:tabplot2) are calculated with following two equations, daily logarithmic return \@ref(eq:LogR) and volatility \@ref(eq:Volatility). [1] [2]

## Logarithmic return
The reason for using logarithmic returns is that the numbers take smaller values than discrete returns. Due to smaller values there is also a smaller variance, which influences fitting positively. As well historical daily data is used for the calculation of the logarithmic return. For later calculations the risk-free return has to be subtracted from the returns. For the sake of simplicity the risk-free rate is already deducted now. For the risk-free return $R_{f}$, shown in table \@ref(tab:tabplot1), the LIBOR interest rate is commonly used. Due to several negative incidents in the past the Swiss Average Rate Overnight (SARON) is used for the time period 2014-2020. SARON is a rate that is based on daily transactions and is therefore considerably more transparent compared to the LIBOR. [3] [4]

\begin{equation}
  R = R_{ln} - R_{f} = ln\left(\frac{x_t}{x_{t-1}}\right) - R_{f}
  \label{eq:LogR}
\end{equation}

<p> *$R$ = return* <p>
<p> *$R_{ln}$ = natural logarithmic return* <p>
<p> *$R_{f}$ = risk-free rate of return* <p>
<p> *$x_{t}$ = closing stock price at time t* <p>

```{r tabplot1}
ann_rf = data.frame(names(rf), round(rf * 252, 3), row.names = NULL)
kable(ann_rf, col.names = c("Year", "Rate [%]"), caption = "Average overnight risk-free return per year in \\%", booktabs = T, linesep = "")
```

## Volatility (standard deviation)
Volatility describes the risk of a stock or market index and is a statistical measure of the dispersion of the calculated logarithmic returns (\@ref(eq:LogR)). In general, the higher the volatility, the riskier the security.

\begin{equation}
  \sigma = \sqrt{\frac{\Sigma(R_{i} - \overline{R})^2}{n}}
  \label{eq:Volatility}
\end{equation}

<p> *$\sigma$ = volatility (standard deviation)* <p>
<p> *$R_{i}$ = each return from the sample* <p>
<p> *$\overline{R}$ = sample mean return* <p>
<p> *$n$ = sample size* <p>

```{r tabplot2}
r = returns
r_mr_vol = data.frame(colnames(r[-1]), mean_returns(r), volatilities(cov_mat(r)), row.names = NULL)
kable(r_mr_vol, col.names = c("Stock","Return [%]", "Volatility [%]"), caption = "SMI with expected return and volatility", digits = 3, booktabs = T, linesep = "")
```

## Covariance and correlation
The covariance is a numerical measure that describes the linear statistical relationship between two variables. In this case, it represents the strength of the relationship between two return time series. The correlation is the standardized covariance. By definition, the correlation ranges in an interval of [-1, 1]. A value of 1 or -1 means that the returns of two shares move in the same direction respectively in opposite directions. In Markowitz' portfolio theory the dependency (correlation) of two individual stocks is an important matter, which will be discussed later on in chapter [Markovitz Model].

\begin{equation}
  cov(x,y) = \frac{\sum_{i = 1}^{n}(x_{i} - \overline{x})(y_{i} - \overline{y})}{n - 1}
  \label{eq:cov}
\end{equation}

<p> *$cov_{x,y}$ = covariance between x and y* <p>
<p> *$x$ = variable x* <p>
<p> *$y$ = variable y* <p>
<p> *$n$ = sample size* <p>

\begin{equation}
  \rho_{i,j} = \frac{cov_{i,j}}{\sigma_{i}*\sigma_{j}}
  \label{eq:Corrcof}
\end{equation}

<p> *$\rho_{i,j}$ = correlation coefficient between return i und j* <p>
<p> *$cov_{i,j}$ = covariance between return i und j* <p>
<p> *$\sigma_{i}$ = volatility of return i* <p>
<p> *$\sigma_{j}$ = volatility of return j* <p>

The matrix below shows the correlations of all 20 stocks included in the SMI. By definition, the diagonal values are always 1. As the SMI only contains shares that are traded on the Swiss stock exchange, there is a rather high correlation between the individual companies. The correlation between companies from the same sector is also higher than that between companies from different sectors or which have less in common. From this it can be concluded that economic similarities are reflected in a higher correlation of stock returns. Another way to demonstrate a connection between two companies is the t-statistic which is described in chapter \@ref(t-statistic).

```{r, fig.cap = "SMI correlation", fig.height = 6}
gg_cor(cor_mat(r), 2, 8, theme = custom_theme_markdown)
```

\newpage

## Statistical key numbers

### T-statistic {#t-statistic}
\begin{equation}
  t = \frac{\rho_{i,j}*\sqrt{n-2}}{\sqrt{1 - \rho_{i,j}^2}}
  \label{sec2}
\end{equation}

<p> *$t$ = t-statistic* <p>
<p> *$\rho_{i,j}$ = correlation coefficient* <p>
<p> *$n$ = sample size* <p>

```{r}
mincor = round(min(abs(cor_mat(r))), 3)
mincorname = row.names(which(round(cor_mat(r),3) == mincor, arr.ind = TRUE))
tvalue = round(cor.test(r[,-1][,mincorname[1]], r[,-1][,mincorname[2]])[[1]], 3)
```

To show that each correlation coefficient is significantly different from zero, the t-test is performed for the two shares with the correlation closest to zero, which are `r mincorname[1]` and `r mincorname[2]`.

\begin{equation}
  t = \frac{`r mincor`*\sqrt{`r nrow(r)`-2}}{\sqrt{1 - `r mincor`^2}} = `r tvalue`
  \label{eq:secr}
\end{equation}

The students t-distribution with `r nrow(r)` degrees of freedom tells us that the probability of getting a test-statistic in the interval of [-`r tvalue`,`r tvalue`] equals 1. Therefore, the probability of getting a test-statistic out of the interval equals 0.
Since the P-value is smaller than 0.05, we can reject the null hypothesis. There is sufficient statistical evidence at the $\alpha$ = 0.05 level to conclude that there is a significant linear relationship between `r mincorname[1]` and
`r mincorname[2]`. 

### Standard error of expected return
In statistics the standard deviation of the sampling distribution is known as the standard error and it provides a statement about the quality of the estimated parameter. The more individual values there are, the smaller is the standard error, and the more accurately the unknown parameter can be estimated. The standard error for the expected return and the volatility are explained in the following two chapters.

\begin{equation}
  \sigma_{\overline{R}} = \frac{\sigma}{\sqrt{n}}
  \label{eq:ser}
\end{equation}

<p> *$\sigma_{\overline{R}}$ = standard error* <p>
<p> *$\sigma$ = standard deviation of sample* <p>
<p> *$n$ = sample size* <p>

```{r gg1, fig.cap = 'Expected mean return with standard error', fig.height = 4}
mr = mean_returns(r)
se_r = apply(r[,-1], 2, se_mean)
min = min(mr - se_r); max = max(mr + se_r)
gg_errorbar(stocks, mr, se_r, c(min, max), "Return [%]", theme = custom_theme_markdown)
```


As can be seen, the standard error of expected mean return by groups do not show larger values as for example the individual shares *Sika* or *Geberit* do. Due to averaging returns the impact of the outliers is stabilized, therefore the standard error declines when grouping individual stocks.

\newpage

### Standard error of volatility (standard deviation)
This formula is an approximation for the standard error of volatility, which is appropriate for n > 10. [8]

\begin{equation}
  \sigma_{\sigma} = \sigma * \frac{1}{\sqrt{2 * (n - 1)}}
  \label{eq:sev2}
\end{equation}

<p> *$\sigma_{\sigma}$ = standard error* <p>
<p> *$\sigma$ = standard deviation* <p>
<p> *$n$ = sample size* <p>

As shown in table \@ref(tab:tabplot5) respectively table \@ref(tab:tabplot6) the values of the standard error of volatility are much lower as the one of the returns. Therefore we will neglect the values for further calculations.

```{r tabplot5}
se_volr = data.frame(colnames(r[-1]), apply(r[,-1], 2, se_sd), row.names = NULL)
kable(se_volr, col.names = c("Stock", "Standard error [%]"), caption = "Standard error of volatility", digits = 5, 
      booktabs = T, linesep = "")
```

\newpage

# Review of classical markovitz optimization
Modern portfolio theory is a theory that deals with the construction of portfolios to maximize the expected return based on a given market risk. Markowitz' portfolio theory shows that efficient risk reduction is only possible if the extent of the correlation of the individual investments is taken into account when putting together the portfolio. Risk reduction through risk diversification is a key finding of the aforementioned portfolio theory. Harry Markowitz pioneered this theory in his article "Portfolio Selection”. The main point is that the risk and return characteristics of an investment should not be considered in isolation, but should be evaluated according to how the investment affects the risk and return of the overall portfolio. It is shown that an investor can construct a portfolio of multiple assets that maximizes returns for a given level of risk. Similarly, an investor can construct a portfolio with the lowest possible risk at a desired level of expected return. Based on statistical measures such as volatility and correlation, the performance of an individual investment is less important than how it affects the portfolio as a whole. [5] 

The return, volatility and covariance matrix is known. The portfolio volatility is accordingly given as a function of the covariance matrix and the weight vector and it can be minimized as much as desired by sufficient diversification. Also the sum of all weights equals 1. The portfolio weights being searched for are described by the vector $\vec{w} = (w_{1}, \dots, w_{n})$. The weights that are calculated are those weights that match the portfolio with minimal volatility (variance) to a given expected portfolio return $R_{p}$. This is a linear optimization problem, as well a formulation of the fundamental problem of balancing return and risk. Furthermore, negative weightings are defined as short sales. [9]


\begin{equation}
minimize: \frac{1}{2}\vec{w}^{T}\Sigma\vec{w}
  \label{eq:minimum}
\end{equation}

*With the following two constraints:*

*I.*
\begin{equation}
1 = \vec{w}^{T}\vec{1}
  \label{eq:constraint1}
\end{equation}

*II.*
\begin{equation}
R_{p} = \vec{w}^{T}\vec{R}
  \label{eq:constraint2}
\end{equation}

According to the method of the Lagrange Multiplier, the Lagrange function is formed with the factors $\lambda$ and $\epsilon$. 

\begin{equation}
L(\vec{w}) = min \frac{1}{2}\vec{w}^{T}\Sigma\vec{w} - \lambda(\vec{w}^{T}\vec{R} - R_{p}) - \epsilon(\vec{w}^{T}\vec{1} - 1)
  \label{eq:lagrangemw}
\end{equation}

The disappearance of the gradient is the necessary condition for a minimum. This is together with the two constraints (\@ref(eq:constraint1), (\@ref(eq:constraint2) an inhomogeneous linear system of equations of the dimension $n + 2$ with $n + 2$ variables. The solution is a known standard problem from linear algebra.

\begin{equation}
\nabla_{w}L = \Sigma\vec{w} - \lambda*\vec{R} - \epsilon*\vec{1} = 0
  \label{eq:gradient}
\end{equation}


<p> *$\vec{w}$ = weight vector* <p>
<p> *$\Sigma$ = covariance matrix* <p>
<p> *$\vec{1}$ = all-ones vector* <p>
<p> *$\vec{R}$ = return* <p>
<p> *$R_{P}$ = total portfolio return* <p>

\newpage

## Problem
Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.

## Minimum variance portfolio 
The Minimum Variance Portfolio, or MVP for short, describes the portfolio of all possible weightings with the minimum volatility. Since only the volatility is minimized, constraint II. is not included in the equation.

\begin{equation}
\vec{w}_{mvp} = \frac{1}{\vec{1}^T\Sigma^{-1}\vec{1}}*\Sigma^{-1}\vec{1}
  \label{eq:mvpanlytic}
\end{equation}

<p> *$\vec{w}_{mvp}$ = weights* <p>
<p> *$\vec{1}$ = all-ones vector* <p>
<p> *$\Sigma$ = covariance matrix* <p>

```{r tabplot7}
mvpw_r = data.frame(colnames(r[-1]), mvp_weights(cov_mat(r)))

kable(mvpw_r, caption = "Weights of MVP", digits = 3, 
      col.names = c("Stock", "Weight"), booktabs = T, linesep = "")
```

It is clear to see that returns which have a negative value also have a negative weighting. In practice, this would lead to a short selling.

## Tangency portfolio 
The tangency portfolio results from the tangent of the capital market line and the efficient frontier, which will be shown more detailed in chapter [Efficient Frontier]. The capital market line is an important component of the Capital Asset Pricing Model. The slope of the capital market line indicates how much more return is expected per additional volatility, therefore a steeper slope of the capital market line gives a better Sharpe ratio. This is exactly the situation when the capital market line is tangential to the efficient frontier. Therefore, the best possible diversified portfolio results from the weightings of the tangency portfolio. 

In this paper the MVP and TP serve as useful reference points to compare the Sharpe ratio of different optimizations. The tangency portfolio is often referred to in the literature as the market portfolio. In the equation for the capital market line, shown as equation \@ref(eq:cml), the expected return of the market portfolio respectively the tangency portfolio is written as $R_{tp}$.

*Capital Market Line:*
\begin{equation}
R_{P}(\sigma_{P}) = R_{f} + \frac{R_{tp} - R_{f}}{\sigma_{tp}} * \sigma_{P}
  \label{eq:cml}
\end{equation}

<p> *$R_{P}$ = total portfolio return as a function of $\sigma_{p}$* <p>
<p> *$R_{f}$ = return of risk-free asset* <p>
<p> *$R_{tp}$ = return of tangency portfolio* <p>
<p> *$\sigma_{P}$ = total portfolio volatility* <p>
<p> *$\sigma_{tp}$ = tangency portfolio volatility* <p>

*Tangency Portfolio:*
\begin{equation}
\vec{w}_{tp} = \frac{1}{\vec{1}\Sigma^{-1}(\vec{R}-R_{f}\vec{1})}*\Sigma^{-1}(\vec{R}-R_{f}\vec{1})
  \label{eq:tananlytic}
\end{equation}

<p> *$\vec{w}_{tp}$ = weights* <p>
<p> *$\vec{1}$ = all-ones vector* <p>
<p> *$\Sigma$ = covariance matrix* <p>
<p> *$\vec{R}$ = return* <p>


```{r tabplot9}
tpw_r = data.frame(colnames(r[-1]), tp_weights(cov_mat(r), mean_returns(r)))

kable(tpw_r, caption = "Weights of tangency portfolio", digits = 3, 
      col.names = c("Stock", "Weight"), booktabs = T, linesep = "")
```

\newpage

## Efficient Frontier
The efficient frontier is a set of points that extends in the return-volatility diagram (figure \@ref(fig:gg3)) between the minimum variance portfolio at the left edge of the reachable area and the tangency portfolio. All possible weightings of portfolios on this line are efficient because they have the maximum return at a defined level of volatility.

\begin{equation}
\vec{w}_{tp} = \alpha * \vec{w}_{mvp} + (1 - \alpha) * \vec{w}_{tp}
  \label{eq:efficientf}
\end{equation}

<p> *$\vec{w}_{tp}$ = weights* <p>
<p> *$\alpha$ = scale factor* <p>
<p> *$\vec{w}_{mvp}$ = weights of minimum variance portfolio* <p>
<p> *$\vec{w}_{tp}$ = weight of tangency portfolio* <p>

\begin{equation}
R_{P} = \vec{w}^T * \vec{R}
  \label{eq:cord}
\end{equation}

<p> *$R_{P}$ = total portfolio return* <p>
<p> *$\vec{w}$ = weights* <p>
<p> *$\vec{R}$ = mean return* <p>

\begin{equation}
\sigma_{P} = \sqrt{\vec{w}^T\Sigma\vec{w}}    
  \label{eq:cord2}
\end{equation}

<p> *$\sigma_{P}$ = total portfolio volatility* <p>
<p> *$\vec{w}$ = weights* <p>
<p> *$\Sigma$ = covariance matrix* <p>


```{r gg3, fig.cap = 'Efficient frontier with stocks', fig.height = 5}
cm = cov_mat(r); mr = mean_returns(r)

efw_r = ef_weights(mvp_weights(cm), tp_weights(cm, mr), seq(-2, 2, 0.1))
efp_r = ef_points(efw_r, cm, mr)

efp = data.frame(n = stocks, vol = volatilities(cm), mr = mr, row.names = NULL)

suppressMessages(suppressWarnings(print(ggplot() +
 geom_abline(intercept = 0, slope = tp_point(efp_r)[2]/tp_point(efp_r)[1], alpha = 0.6, col = "burlywood4") +
 geom_path(aes(x = efp_r[,1], y = efp_r[,2]), alpha = 0.5) +
 geom_point(aes(x = mvp_point(efp_r)[1], y = mvp_point(efp_r)[2], color = "MVP")) +
 geom_point(aes(x = tp_point(efp_r)[1], y = tp_point(efp_r)[2], color = "TP")) +
 geom_text_repel(data = efp, aes(x = vol, y = mr, label = n), size = 3) +
 lims(x = c(0, 3), y = c(-0.1, 0.2)) +
 labs(x = "Volatility [%]", y = "Expected return [%]", color = NULL) +
 scale_color_manual(values = c("MVP" = "cornflowerblue", "TP" = "orangered3")) +
 guides(fill=guide_legend(ncol = 2)) +
 theme(legend.position = "bottom") +
 custom_theme_markdown)))
```

In figure \@ref(fig:gg3) it is clearly visible that the SMI shows a significantly higher return with almost the same volatility and vice versa. This leads to a larger in sample Sharpe ratio of the SMI compared to the groups. The reason for a higher in sample Sharpe ratio of the individual stocks is due to the higher amount of data. There are 20 stocks in the SMI for which the weighting can be optimized, but only four in the grouping. However, it will become apparent in chapter \@ref(cross) that this no longer applies to the out of sample Sharpe ratio, which in our case is much more important than the in sample.

\newpage

## Sharpe ratio
The Sharpe ratio measures the performance of an investment which means the return of an investment compared to its risk. Generally, the greater the value of the Sharpe ratio, the more attractive the risk-adjusted return. In practice, the value of the Sharpe Ratio is not only positively received. In this paper, however, we will rely on the Sharpe ratio because it is a key figure by which performance can be measured. It is calculated by the average return earned in excess of the risk-free rate per unit of volatility. In this case it is the natural logarithmic return per day $R$ divided by $\sigma$. $R$ is already calculated in equation \@ref(eq:LogR). [6] [7]

\begin{equation}
  Sharpe ratio = \frac{R_{ln} - R_{f}}{\sigma} = \frac{R}{\sigma}
  \label{eq:sharperatio}
\end{equation}

<p> *$R$ = natural logarithmic return per day* <p>
<p> *$R_{ln}$ = mean logarithmic return* <p>
<p> *$R_{f}$ = risk-free rate of return* <p>
<p> *$\sigma_{i}$ = volatility of return* <p>

Before any weighting of a portfolio is optimized, the Sharpe ratio of the SMI is calculated. This is done by weighting all stocks equally, 1/20 each, and then applying the weightings to the actual returns of the historical data. This results in a Sharpe ratio of XX. Note that the Sharpe ratio is annualized, which means that it is multiplied by $\sqrt{252}$. As mentioned above, the Sharpe ratio is used as a measure of the optimization and robustness. A robust optimization leads to a lower volatility which results in a higher Sharpe ratio. The higher the value the better the optimization.

# Methodology


## Grouping
A first way to increase the Sharpe ratio is to create groups. Grouping the stocks also reduces the standard errors, which will be described in more detail later in chapter [Standard error]. 
Naturally there are different approaches how many groups and how exactly the composition of such groups should be created. In a first approach we formed four groups, which differ in their industries:

<p> *Consumer: Adecco, Nestle, Richemont, Swatch, Swisscom* <p>
<p> *Finance: Credit Suisse, Julius Baer, Swiss Life, Swiss Re, UBS, Zurich Insurance* <p>
<p> *Industrial: ABB, Geberit, Givaudan, Lafarge Holcim, SGS* <p>
<p> *Pharma: Lonza, Novartis, Roche, Sika* <p>

This method is not mathematical in nature but a simple intuitive decision based on the perception of these companies. Within the grouping, each share is equally weighted. This combination results in the following values for the mean return and volatility of each group as shown in table \@ref(tab:tabplot3).

```{r tabplot3}
gr = groups_returns(returns, groups)
gr_mr_vol = data.frame(colnames(gr[-1]), mean_returns(gr), volatilities(cov_mat(gr)), row.names = NULL)
kable(gr_mr_vol, col.names = c("Group","Return [%]", "Volatility [%]"), caption = "groups with expected return and volatility", digits = 3, booktabs = T, linesep = "")
```

It can already be seen that the values of volatility are lower overall than those of the individual shares, which leads to a higher Sharpe ratio as shown in table \@ref(tab:tabplot4). The Sharpe ratio of groups increased significantly compared to the Sharpe Ratio for the SMI. 
In the matrix below, the correlation of the individual groups is shown. As the returns are averaged within the groups, the values of the individual groups converge, resulting in a larger correlation coefficient.

```{r, fig.cap = "Groups correlation", fig.height = 6}
gg_cor(cor_mat(gr), 4, 12, theme = custom_theme_markdown)
```

\newpage


## Bootstrap
In this part a bootstrap process was made to analyze the robustness of the tangency portfolio weights when resampling all historical returns.

All dates from the historical data were sampled in the same dimension, with replacing included. Which means that certain dates can occur multiple times. From each sampled date, the corresponding returns of each stock are added to the new data, consequently maintaining the daily differences between stocks. If replacing would not be included in the sampling method, the correlation matrix would remain constant to the one of the original data.

One Bootstrap sample contains a different data, with which a new mean return of each stock and group and the corresponding covariance matrix is calculated. Those two variables are needed as input in the optimization function to calculate minimum variance portfolio, tangency portfolio of the 20 stocks and the four groups. With 100 bootstrap samples, 100 MVP’s and TP’s can be compared and analyzed. Additionally, the standard deviation of the weights over those number of bootstrap samples can be studied. The standard deviation is computed with the 84-Quantil minus the 50-Quantil to achieve a more robust result, because this calculation is less vulnerable to outliers.

```{r gg4, fig.cap = 'Bootstrap samples efficiency frontier', fig.height = 3.8}
bs_r = bootstrap(r); bs_gr = bootstrap(gr)
suppressMessages(suppressWarnings(print(gg_bootstrap_ef(bs_r$samples_ef_points, theme = custom_theme_markdown))))
```

In figure \@ref(fig:gg4) all bootstrap samples can be seen with their minimum variance portfolio, tangency portfolio and their efficiency frontier. Noticeable is the variance of the tangency portfolio in comparison to the minimum variance portfolio. This is due to the high standard errors of returns. Since the return is included in the calculation of the TP, but not in the MVP, a larger deviation can be seen.

```{r gg9, fig.cap = 'Sample tangency portfolio stock weights', fig.height = 3}
# stock1 = "ABB"
# stock2 = "Adecco"
# 
# ggplot(bs_out_SMI$sample_weights_tp) +
#   geom_point(aes_string(x = paste0("`", stock1, "`"), y = paste0("`", stock2, "`")),
#              color = "orangered3") +
#   labs(x = stock1, y = stock2) +
#   custom_theme_markdown
```

\newpage

## Cross validation {#cross}
This chapter describes the methodology to measure the performance of the Markovitz model.

The measure is the previously described Sharpe ratio and in particular the out of sample Sharpe ratio is of interest. Out of sample means that the model is not tested on the data which created it but on an independent dataset instead. Contrary, in sample means that the model is tested on the data which created it. A widely used method for out of sample testing is the so-called cross validation.

In our case, as shown in figure \@ref(fig:crossvalidation), historical data which ranges from 2001 to 2020 is split into five different sections and consequentially five different models will be developed. The red sections are the training sets and the blue ones the testing sets.

```{r crossvalidation, fig.cap = "Cross validation test and training sets", out.width = "95%"}
knitr::include_graphics("./www/Cross_validation.jpg")
```

In the case of the Markovitz model, this means that the returns, volatility and covariance of the training sets are used to calculate the weights which are then applied to the testing set returns. By the end of the cross validation, one can combine all five testing sets into one time series which equals the length of the original one, but is out of sample. This out of sample time series is used to calculate the Sharpe ratio. As a comparison, the in sample Sharpe ratio is also calculated as shown in table \@ref(tab:tabplot11).

As one might expect, the optimization performs better in sample and therefore the in sample Sharpe ratio is higher than the out of sample Sharpe ratio.

```{r gg99, fig.cap = "Tangency portfolio weights by model", fig.height = 5}
is_r = in_sample(r); is_gr = in_sample(gr)
sets_r = cross_validation_sets(r); sets_gr = cross_validation_sets(gr)
os_r = out_of_sample(sets_r); os_gr = out_of_sample(sets_gr)

sets_tpw = matrix(ncol = 5, nrow = 20)

for (i in 1:5) {
  sets_tpw[,i] = out_of_sample(sets_r, set = i)$tp_weights
}

df = data.frame(order = stocks, sets_tpw)

alpha = 0.2
ggplot(df) + geom_line(aes(x = order, y = X1, group = 1, col = "Model 1"), alpha = alpha) +
  geom_point(aes(x = order, y = X1, group = 1, col = "Model 1")) +
  geom_line(aes(x = order, y = X2, group = 2, col = "Model 2"), alpha = alpha) +
  geom_point(aes(x = order, y = X2, group = 2, col = "Model 2")) +
  geom_line(aes(x = order, y = X3, group = 3, col = "Model 3"), alpha = alpha) +
  geom_point(aes(x = order, y = X3, group = 3, col = "Model 3")) +
  geom_line(aes(x = order, y = X4, group = 4, col = "Model 4"), alpha = alpha) +
  geom_point(aes(x = order, y = X4, group = 4, col = "Model 4")) +
  geom_line(aes(x = order, y = X5, group = 5, col = "Model 5"), alpha = alpha) +
  geom_point(aes(x = order, y = X5, group = 5, col = "Model 5")) +
  labs(x = NULL, y = "Weight", color = NULL) +
  scale_color_manual(values = c("Model 1" = "cornflowerblue", "Model 2" = "orangered3",
                                "Model 3" = "lightblue4", "Model 4" = "forestgreen",
                                "Model 5" = "tan2")) +
  theme(axis.text.x = element_text(angle = 90)) +
  guides(fill=guide_legend(ncol = 5)) +
  theme(legend.position = "bottom") +
  custom_theme_markdown
```

\newpage

## Shrinkage {shrink}
One method for improving the out of sample Sharpe ratio is shrinking. This approach focuses on altering the returns and correlation prior to the optimization with the goal to obtain a better model in regard to the out of sample Sharpe ratio.

### Shrinking factor for return
The formula for the return shrinking factor is shown in \@ref(eq:shrinkR). When the shrinking factor is set to zero, the returns are not altered and remain the same. When increasing the shrinking factor from zero up to one, the returns converge towards their mean value.

\begin{equation}
R_{i}(\lambda) = \lambda*R_{i} + (1 - \lambda) * \overline{R}
  \label{eq:shrinkR}
\end{equation}

<p> *$R_{i}(\lambda)$ = return of stock i as a function of $\lambda$* <p>
<p> *$\lambda$ = shrinking factor* <p>
<p> *$R_{i}$ = return of stock i* <p>
<p> *$\overline{R}$ = mean return of all stocks* <p>

### Shrinking factor for correlation
The formula for the correlation shrinking factor is shown in \@ref(eq:shrinkCor). The reason for shrinking the correlation instead of the covariance is that the former is standardized and therefore more suitable. When the shrinking factor is set to one, the correlation is not altered and remains the same. When decreasing the shrinking factor from one to zero, the correlation converges towards zero. This means that all entries of the correlation matrix are equal to zero except the diagonal, which is by definition always equal to one.

\begin{equation}
\rho_{i,j} (\epsilon) = I_{i,j} + \epsilon * \tilde{\rho_{i,j}}
  \label{eq:shrinkCor}
\end{equation}

<p> *$\rho_{i,j} (\epsilon)$ = correlation coefficient as a function of $\epsilon$* <p>
<p> *$\epsilon$ = shrinking factor* <p>
<p> *$I_{i,j}$ = identity matrix* <p>
<p> *$\tilde{\rho_{i,j}}$ = correlation matrix with diagonal zero* <p>

### Shrinking factors analyzed separately
The Sharpe ratio can be visualized as a function of the shrinking factor. Figure \@ref(fig:gg20) shows such a plot for the return shrinking factor. The global maxima are highlighted with points and their coordinates. It can be seen that the highest Sharpe ratio for the SMI is achieved with a shrinking factor of about two thirds. For the groups on the other hand shrinking does not lead to an improvement of the Sharpe ratio, as the highest Sharpe ratio is achieved at shrinking factor of zero which means that the returns are not altered. For the SMI it is visible that close to the shrinking factor one the line is interrupted. This implies that the corresponding Sharpe ratio is below zero which is not shown in the plot. These outliers are caused by extreme portfolio weights which are obtained at these shrinking factors. Extreme portfolio weights are obtained through near-singularity of the covariance matrix.

```{r gg20, fig.cap = 'Sharpe ratio as a function of return shrinking factor',fig.height = 3}
os_r_sr = unlist(out_of_sample_vec(sets_r, seq(0, 1, 0.01)))
os_gr_sr = unlist(out_of_sample_vec(sets_gr, seq(0, 1, 0.01)))

gg_shrinking2D(os_r_sr, os_gr_sr, "SMI", "Groups", "Return", theme = custom_theme_markdown)
```

Figure \@ref(fig:gg21) shows the same plot for the correlation shrinking factor. The highest Sharpe ratio for the SMI is achieved with a shrinking factor of about one third and for the groups of close to one.

```{r gg21, fig.cap = 'Sharpe ratio as a function of correlation shrinking factor', fig.height = 3}
os_r_scor = unlist(out_of_sample_vec(sets_r, 1, seq(0, 1, 0.01)))
os_gr_scor = unlist(out_of_sample_vec(sets_gr, 1, seq(0, 1, 0.01)))

gg_shrinking2D(os_r_scor, os_gr_scor, "SMI", "Groups", "Correlation", theme = custom_theme_markdown)
```

It can be concluded that for the SMI shrinking does make a noticeable difference in regard of increasing the Sharpe ratio. It is to note that the shrinking factors of the return and correlation are equal in regard of the deviation from the original values. The optimal return shrinking factor is about two thirds (no shrinking at value 0) and the one of the correlation about one third (no shrinking at value 1) which is in both cases a deviation of about two thirds from the original values.

In the case of the groups shrinking does not have proven to make a noticeable impact. Shrinking the returns does worsen the Sharpe ratio and Shrinking the correlation does only make a slight difference.
The conclusion of the SMI above also holds true here, as both shrinking factors deviate approximately the same from their original values, which in this case is no or almost no deviation.

### Shrinking factors analyzed simultaneously
A further visualization is a three dimensional plot where the Sharpe ratio is plotted as a function of both shrinking coefficients. The points of the maxima and their values are also displayed.

Figure \@ref(fig:gg22) shows such a plot for the SMI. The outliers where the Sharpe ratio drops below zero can also be seen in this plot in the top left corner, although only on the lower third of the correlation shrinking factor. The highest Sharpe ratio is achieved at a correlation shrinking factor of about one third again but a return shrinking factor of zero meaning the original values are used. Varying both shrinking factors together yields the same Sharpe ratio as in \@ref(fig:gg21) where only the correlation shrinking factor is varied. Noticeable is that the major part of the plot area has Sharpe ratios which are very close to each other.

```{r gg22, fig.cap = 'SMI Sharpe ratio as a function of return and correlation shrinking factor', fig.height = 4}
grid = expand.grid(seq(0, 1, by = 0.05), seq(0, 1, by = 0.05))
os_r_sr_scor = unlist(out_of_sample_vec(sets_r, grid[,1], grid[,2]))

gg_shrinking3D(grid, os_r_sr_scor, theme = custom_theme_markdown)
```

Figure \@ref(fig:gg23) shows the three dimensional plot for the groups. Here, a much smaller fraction of the plot area shows Sharpe ratios which are similarly high. The highest Sharpe ratio is achieved at a correlation shrinking factor of close to one and a return shrinking factor of zero. Varying both shrinking factors togehther yields also the same Sharpe ratio as in \@ref(fig:gg21) wher only the correlation shrinking factor is varied.

```{r gg23, fig.cap = 'Groups Sharpe ratio as a function of return and correlation shrinking factor', fig.height = 4}
os_gr_sr_scor = unlist(out_of_sample_vec(sets_gr, grid[,1], grid[,2]))

gg_shrinking3D(grid, os_gr_sr_scor, theme = custom_theme_markdown)
```

To conclude these results, it can be said that shrinking of the correlation has a greater impact than shrinking of the return and can result in higher Sharpe ratios, although not in all cases. Table \@ref(tab:tabplot20) provides an overview.

```{r tabplot20}
t20 = data.frame(c(is_r, is_gr),
                 c(os_r, os_gr),
                 c(max(os_r_sr_scor), max(os_gr_sr_scor)), row.names = c("SMI", "Groups"))

kable(t20, caption = "Sharpe ratio",  digits = 3,
       booktabs = T, linesep = "", col.names = c("In sample", "Out of sample", "Out of sample shrinking"))
```

\newpage

# Results
In the results the different approaches of the mentioned methodology is described. The first section is about the outcome of grouping and shrinkage measured with sharpe ratio and the second section about the result of the sampling method with bootstrapping.


## I.
The two tables below visualize the different sharpe ratio outcome. Firstly the table \@ref(tab:tabplot4) displays the sharpe ratio of SMI and Group returns when all have an equal weight and only the method of grouping is applied. Also no optimization is used. The second table \@ref(tab:tabplot11) displays the results of the shrinkage section \@ref(shrink) with markovitz optimization and grouping.

```{r tabplot4}
weights_SMI = rep(1/20, 20)
test1 = as.matrix(r[,-1]) %*% weights_SMI
sr_SMI = ann_sharpe_ratio(mean(test1) / sd(test1), "1d")

weights_groups = rep(1/4, 4)
test2 = as.matrix(gr[,-1]) %*% weights_groups
sr_groups = ann_sharpe_ratio(mean(test2) / sd(test2), "1d")

t4 = data.frame(t(c(sr_SMI, sr_groups)))
 
kable(t4, caption = "sharp ratio",  
      booktabs = T, linesep = "", col.names = c("Sharpe ratio of SMI", "Sharpe ratio of groups"))
```
If no method is applied the sharpe ratio is very low. The first observation which can be made is, that the grouping method leads to a better sharpe ratio and therefore the method proves its worth if no optimization is performed.

```{r tabplot11}
t11 = data.frame(c(is_r, is_gr), c(os_r, os_gr), row.names = c("SMI", "Groups"))

kable(t11, caption = "Sharpe ratio",  digits = 3, 
       booktabs = T, linesep = "", col.names = c("In sample", "Out of sample"))
```



## II.

In the following four illustrations the results of the bootstrap can be seen. In all four graphics the mean weights and the standard deviation over the 100 samples are shown. Figure \@ref(fig:gg5) and figure \@ref(fig:gg6) visualize the MVP weights by all 20 stocks and by group. In comparison to figure \@ref(fig:gg7) and figure \@ref(fig:gg8) where the same is depicted with the tangency portfolio.

```{r}
mvpw_r = mvp_weights(cov_mat(r)); mvpw_gr = mvp_weights(cov_mat(gr))
mvpw_sd_r = bs_r$mvp_weights_sd; mvpw_sd_gr = bs_gr$mvp_weights_sd
min = min(c(mvpw_r - mvpw_sd_r, mvpw_gr - mvpw_sd_gr))
max = max(c(mvpw_r + mvpw_sd_r, mvpw_gr + mvpw_sd_gr))
gg3 = gg_errorbar(stocks, mvpw_r, mvpw_sd_r, c(min, max), "Weights", theme = custom_theme_markdown)
gg4 = gg_errorbar(colnames(gr[-1]), mvpw_gr, mvpw_sd_gr, c(min, max), "Weights", theme = custom_theme_markdown)
plots34 = align_plots(gg3, gg4, align = "h")
```

```{r gg5, fig.cap = 'MVP weights with standard error by SMI', fig.height = 4}
ggdraw(plots34[[1]])
```

```{r gg6, fig.cap = 'MVP weights with standard error by group', fig.height = 4}
ggdraw(plots34[[2]])
```

```{r}
tpw_r = tp_weights(cov_mat(r), mean_returns(r)); tpw_gr = tp_weights(cov_mat(gr), mean_returns(gr))
tpw_sd_r = bs_r$tp_weights_sd; tpw_sd_gr = bs_gr$tp_weights_sd
min = min(c(tpw_r - tpw_sd_r, tpw_gr - tpw_sd_gr))
max = max(c(tpw_r + tpw_sd_r, tpw_gr + tpw_sd_gr))
gg5 = gg_errorbar(stocks, tpw_r, tpw_sd_r, c(min, max), "Weights", theme = custom_theme_markdown)
gg6 = gg_errorbar(colnames(gr[-1]), tpw_gr, tpw_sd_gr, c(min, max), "Weights", theme = custom_theme_markdown)
plots56 = align_plots(gg5, gg6, align = "h")
```

```{r gg7, fig.cap = 'tangency portfolio weights with standard error by SMI', fig.height = 4}
ggdraw(plots56[[1]])
```

```{r gg8, fig.cap = 'Tangency portfolio weights with standard error by group', fig.height = 4}
ggdraw(plots56[[2]])
```


As it is obvious to see and also visualized in figure \@ref(fig:gg9), is that TP weights have a higher standard deviation than MVP weights. That means the TP is more sensible to changes of returns than the MVP is. If the equation \@ref(eq:mvpanlytic) of MVP and the equation \@ref(eq:tananlytic) of TP are considered, the MVP does not depend on the returns directly. Consequently different returns samples have not a large impact on the MVP. The small differences arise from the varying covariance matrix. But it can also be noticed that this matrix has not large deviations following the standard deviation of the MVP. Therefore bootsptrap sampling of the returns does not influence the covariance matrix as much as the returns itself.

## III.



# Conclusion



\newpage

# References

[1] Yahoo Finance, SMI Equities - EBS in CHF [online]. Available from:
https://finance.yahoo.com. [Accessed 28 October 2020]

[2] SIX Group, SMI® – the Blue-Chip Index [online]. Available from:
https://www.six-group.com/exchanges/indices/data_centre/shares/smi_en.html [Accessed 15 October 2020]

[3] SIX Group, Swiss Reference Rates (SARON) [online]. Available from:
https://www.six-group.com/exchanges/indices/data_centre/swiss_reference_rates/reference_rates_en.html [Accessed 30 October 2020]

[4] global-rates, CHF LIBOR interest rate [online]. Available from:
https://www.global-rates.com/en/interest-rates/libor/swiss-franc/swiss-franc.aspx [Accessed 28 October 2020]

[5] H. Makovitz (1952). Portfolio Selection.
The Journal of Finance Vol. 7, No. 1, pp. 77-91 (15 pages). Published By: Wiley.

[6] W. F. Sharpe (1966). Mutual Fund Performance.
The Journal of Business, Vol. 39, No. 1, Part 2: Supplement on Security Prices, pp. 119-138. 
Published by: The University of Chicago Press,

[7] A. W. Lo (2003). The Statistics of Sharpe Ratios.
Sloan School of Management, Massachusetts Instintute of Technology, Cambridge.

[8] Ahn, S. and Fessler, J. (2003). Standard Errors of Mean, Variance and, Standard Deviation Estimators. 
EECS Department, The University of Michigan, pp.1-2.

[9] D. G. Leuenberger (1998). Investment Science. 
Published by: Oxford University Press, New York.

\newpage

\listoffigures

\listoftables
