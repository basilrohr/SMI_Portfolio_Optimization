---
title: "Robust Methods of Portfolio Optimization Exemplified by the Swiss Market Index Constituents\\vspace{0.1cm}"
subtitle: "School of Engineering, Zurich University of Applied Sciences"
author:
  - Pascal Aigner
  - Maurice Gerber
  - Basil Rohr
date: "`r format(Sys.time(), '%d. %B %Y')`"
abstract: "Markowitz optimization requires estimates of mean return, standard deviation, and correlation of individual stocks, all of which are subject to estimation error. This leads to the risk of overfitting historical data and, in the case of portfolio optimization, creating portfolios that perform well in backtesting but poorly in the future. To overcome these problems, several robust approaches are investigated, such as different asset grouping methods and shrinkage factors for mean return and correlation. The goal is to provide a recommended approach for optimizing a portfolio that includes all assets from the Swiss Market Index. In summary, three key observations are made. First, asset grouping leads to significant noise reduction and thus better portfolio performance compared to individual assets. Likewise, shrinkage leads to increased performance due to major noise reduction. Using both methods simultaneously achieves a further, albeit smaller, increase in performance, as noise can only be reduced to a certain extent. All code is published on \\url{https://github.com/pascalaigner/markovitz} and the Shiny app can be accessed at \\url{https://pascalaigner.shinyapps.io/markovitz}.\\par\\textbf{Keywords:} Portfolio optimization, Markovitz model, Robust methods, Shrinkage\\newpage"
output:
  bookdown::pdf_book:
    toc: false
classoption: twocolumn
geometry: margin = 20mm
documentclass: extarticle
fontsize: 8pt
bibliography: references/references.bib
csl: references/ieee.csl
link-citations: true
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \floatplacement{table}{H}
  - \setlength{\abovecaptionskip}{0pt}
---

```{r, include = F}
knitr::opts_chunk$set(echo = F)
library(knitr)
library(ggplot2)
library(ggcorrplot)
library(ggrepel)
library(heatmap3)
library(ggpubr)
library(dplyr)

load("./Data/data_1d.Rda")
load("./Data/rf_1d.Rda")
load("./Data/returns_1d.Rda")
R.utils::sourceDirectory("./Code", modifiedOnly = F)
```

# Introduction
In order to make the best possible use of returns and volatilities when selecting assets in a portfolio, the US economist Harry M. Markowitz developed a mathematical method in 1952 to determine efficient portfolios. This method has since been known as modern portfolio theory. Since then, it is formalized how to create the most efficient portfolio from different stocks with given variables such as returns and volatilities. The disadvantage of this optimization is that the calculations are based on historical data. This data, namely the returns, are strongly affected by noise and therefore will not perform well in future estimations. 

The fundamental question is how to create an optimization from historical data that delivers robust results on independent data. In this paper, the Sharpe ratio is used as a performance measurement of the optimization. The data used in this paper are the assets included in the SMI, afterwards a short review of Markovitz theory is presented. To achieve more robust results, several approaches are investigated. The first method described is grouping where a qualitative and a quantitative approach are applied and compared. Followed by a description of the bootstrap which is used to determine the standard errors of the asset weights. To measure the performance on to an independent data set, cross validation is applied to obtain an out of sample time series. As a final approach, shrinkage of the mean returns and correlations is discussed. 

All these methods are applied to 20 years of historical data and their performance in terms of Sharpe ratio is compared. At the end of this paper, the aim is to provide a general approach to create a robust portfolio optimization.

\newpage

# Data
In this paper, daily historical data from the Swiss Market Index, short SMI, and its constituents is used. The data stems from the period `r format(df[,1][1], "%d.%m.%Y")` to `r format(df[,1][nrow(df)], "%d.%m.%Y")`. The SMI is the most important stock index in Switzerland and contains the 20 largest companies traded on the Swiss stock exchange. It covers approximately 80% of the total market capitalization of the Swiss stock market. The SMI is also a price index which means that dividends are not included. It is reviewed twice a year and, if necessary, reassembled. In this case, the composition per end of 2018 is used. The reason for the 2018 composition is that enough historical data for all companies is available. The key numbers are shown in table \@ref(tab:rmrvoltab). [@yahoo], [@six]

## Logarithmic return
The reason for calculating logarithmic returns is that stock returns are generally assumed to be log-normally and not normally distributed. In later calculations, the risk-free rate of return has to be subtracted from the return and for the sake of simplicity, this is already done from the start as illustrated in eq. \@ref(eq:logr). For the risk-free rate $R_{f}$ shown in table \@ref(tab:annrftab), the LIBOR is commonly used. Due to several negative incidents in the past, the Swiss Average Rate Overnight (SARON) is used for the time period 2014 to 2020. The SARON is based on daily transactions and is therefore considerably more transparent compared to the LIBOR. [@saron], [@libor]

\begin{equation}
R = R_{ln} - R_{f} = ln\left(\frac{x_{t}}{x_{t-1}}\right) - R_{f}
\label{eq:logr}
\end{equation}

<p> *$R$ = return* <p>
<p> *$R_{ln}$ = natural logarithmic return* <p>
<p> *$R_{f}$ = risk-free rate of return* <p>
<p> *$x_{t}$ = closing stock price at time t* <p>

```{r annrftab}
ann_rf = data.frame(names(rf), rf * 252, row.names = NULL)
kable(ann_rf, col.names = c("Year", "Rate [%]"),
      caption = "Annualized risk-free rate of return",
      digits = 3, booktabs = T, linesep = "")
```

\newpage

## Volatility (standard deviation)
The volatility in eq. \@ref(eq:vol) describes the risk of an asset and is a statistical measure of the dispersion of its returns. In general, the higher the volatility, the riskier the asset.

\begin{equation}
\sigma = \sqrt{\frac{\sum_{i = 1}^{n}(R_{i} - \overline{R})^2}{n-1}}
\label{eq:vol}
\end{equation}

<p> *$\sigma$ = volatility (standard deviation)* <p>
<p> *$R$ = return* <p>
<p> *$\overline{R}$ = mean return* <p>
<p> *$n$ = sample size* <p>

```{r rmrvoltab}
r = returns
r_mr_vol = data.frame(colnames(r[-1]), mean_returns(r), volatilities(cov_mat(r)), row.names = NULL)
kable(r_mr_vol, col.names = c("Stock","Return [%]", "Volatility [%]"),
      caption = "Mean return and volatility of SMI constituents",
      digits = 3, booktabs = T, linesep = "")
```

## Covariance and correlation
The covariance in eq. \@ref(eq:cov) is a numerical measure that describes the linear relationship between two variables. In this case, it represents the strength of the relationship between two return time series. The correlation in eq. \@ref(eq:cor) is the standardized covariance and ranges in an interval of $[-1, 1]$ with the diagonal elements of a correlation matrix always being one. A value of one or minus one means that the returns of two assets move in the same direction respectively in opposite directions. In Markowitz' portfolio theory the correlation of two assets is an important matter which will be discussed later in chapter \@ref(markovitz).

\newpage

\begin{equation}
cov_{x,y} = \frac{\sum_{i = 1}^{n}(x_{i} - \overline{x})(y_{i} - \overline{y})}{n - 1}
\label{eq:cov}
\end{equation}

<p> *$cov_{x,y}$ = covariance between returns of assets x and y* <p>
<p> *$x$ = return of asset x* <p>
<p> *$y$ = return of asset y* <p>
<p> *$n$ = sample size* <p>

\begin{equation}
\rho_{i,j} = \frac{cov_{i,j}}{\sigma_{i}*\sigma_{j}}
\label{eq:cor}
\end{equation}

<p> *$\rho_{i,j}$ = correlation between returns of assets i and j* <p>
<p> *$cov_{i,j}$ = covariance between returns of assets i and j* <p>
<p> *$\sigma_{i}$ = volatility of return of asset i* <p>
<p> *$\sigma_{j}$ = volatility of return of asset j* <p>

The correlation matrix of all 20 stocks is shown in figure \@ref(fig:corplot). As the SMI only contains stocks that are traded on the Swiss stock exchange, there is generally a rather high correlation between them.

```{r corplot, fig.cap = "Correlation between SMI constituents", fig.height = 6}
suppressWarnings(suppressMessages(print(gg_cor(cor_mat(r), 2, 8, theme = custom_theme_markdown))))
```

\newpage

## Statistical key numbers

### t-Test
A t-test for the correlation as in eq. \@ref(eq:ttest) is conducted to investigate if the true correlation is significantly different from zero.

\begin{equation}
t = \frac{\rho_{i,j}*\sqrt{n-2}}{\sqrt{1-\rho_{i,j}^2}}
\label{eq:ttest}
\end{equation}

<p> *$t$ = test statistic* <p>
<p> *$\rho_{i,j}$ = correlation between returns of assets i and j* <p>
<p> *$n$ = sample size* <p>

The null hypothesis says that the correlation is not significantly different from zero and the alternative hypothesis that it is significantly different from zero.

<p> *$H_{0}:$ $\rho = 0$* <p>
<p> *$H_{1}:$ $\rho \neq 0$* <p>

```{r}
min_cor = round(min(abs(cor_mat(r))), 3)
min_cor_names = row.names(which(round(cor_mat(r),3) == min_cor, arr.ind = T))
tstatistic = round(cor.test(r[,-1][,min_cor_names[1]], r[,-1][,min_cor_names[2]])[[1]], 3)
```

The test is performed for the two stocks with the smallest correlation which are *`r min_cor_names[1]`* and *`r min_cor_names[2]`*. When inserting the numbers in eq. \@ref(eq:ttest) as shown in eq. \@ref(eq:tteststatistic), one obtains `r tstatistic`.

\begin{equation}
t = \frac{`r min_cor`*\sqrt{`r nrow(r)`-2}}{\sqrt{1-`r min_cor`^2}} = `r tstatistic`
\label{eq:tteststatistic}
\end{equation}

With the help of the t-distribution in figure \@ref(fig:tdist) it is seen clearly that a value of approximately 2 is already below the threshold of $\alpha = 0.05$. Therefore, with the obtained test statistic value of `r tstatistic` which is far to the right in figure \@ref(fig:tdist), the p-value is almost zero. Consequentially, the null hypothesis can be rejected, meaning that the true correlation is significantly different from zero.

```{r tdist, fig.cap = paste0("t-distribution with ", nrow(r), " $-$ 2 degrees of freedom"), fig.height = 3}
seq = seq(-5, 5, 0.01)
ggplot() +
  geom_line(aes(x = seq, y = dt(seq, nrow(r)))) +
  geom_hline(yintercept = 0.05, col = "orangered3", linetype = "dashed") +
  labs(x = "x", y = "P(x)") +
  custom_theme_markdown
```

### Standard error of mean return
The standard deviation of the sampling distribution is known as the standard error and it provides a statement about the quality of the estimated parameter. The more observations there are, the smaller the standard error is and the more accurately the unknown parameter can be estimated. The standard error of mean return is shown in eq. \@ref(eq:semr).

\begin{equation}
\sigma_{\overline{R}} = \frac{\sigma}{\sqrt{n}}
\label{eq:semr}
\end{equation}

<p> *$\sigma_{\overline{R}}$ = standard error of mean return* <p>
<p> *$\sigma$ = standard deviation* <p>
<p> *$n$ = sample size* <p>

\newpage

Figure \@ref(fig:serplot) depicts the standard error of mean return for each stock. It is seen clearly that stocks as *Nestle* and *Swisscom* with a low volatility in table \@ref(tab:rmrvoltab) also have a lower standard error. Likewise, *ABB* and *Credit Suisse* with a high volatility show a higher standard error.

```{r serplot, fig.cap = "Standard error of mean return", fig.height = 4}
mr = mean_returns(r)
se_r = apply(r[,-1], 2, se_mean)
min = min(mr - se_r); max = max(mr + se_r)
gg_errorbar(stocks, mr, se_r, c(min, max), "Return [%]", theme = custom_theme_markdown)
```

### Standard error of volatility (standard deviation)
Eq. \@ref(eq:sevol) is an approximation for the standard error of volatility which is appropriate for n > 10. [@se]

\begin{equation}
\sigma_{\sigma} = \frac{\sigma}{\sqrt{2 * (n - 1)}}
\label{eq:sevol}
\end{equation}

<p> *$\sigma_{\sigma}$ = standard error of volatility (standard deviation)* <p>
<p> *$\sigma$ = volatility (standard deviation)* <p>
<p> *$n$ = sample size* <p>

As shown in table \@ref(tab:sevolrtab), the standard errors of volatility are much lower than the ones of the mean returns in figure \@ref(fig:serplot). This property will be clearly visible in chapter \@ref(bootstrap). Furthermore, the standard errors of volatility are neglected for further investigations as they can be estimated almost exactly.

```{r sevolrtab}
se_volr = data.frame(colnames(r[-1]), apply(r[,-1], 2, se_sd), row.names = NULL)
kable(se_volr, col.names = c("Stock", "Standard error [%]"),
      caption = "Standard error of volatility",
      digits = 5, booktabs = T, linesep = "")
```

\newpage

# Review of classical markovitz optimization {#markovitz}
Modern portfolio theory deals with the construction of portfolios that maximize return for a given level of risk. Harry Markowitz pioneered this theory in his article *Portfolio Selection*. His portfolio theory shows that efficient risk reduction is only possible if the degree of correlation between the individual assets is considered in the construction of the portfolio. Risk reduction through risk diversification is a central insight of the aforementioned theory. The main point is that risk and return characteristics of an asset should not be considered in isolation but be judged by how the asset affects risk and return of the whole portfolio. It is shown that an investor can construct a portfolio of multiple assets that maximizes return for a given level of risk. Likewise, an investor can construct a portfolio with the lowest possible risk for a desired level of return. [@markovitz]

It is assumed that return and covariance are known. The portfolio volatility is given as a function of covariance matrix and weights vector and can be minimized by sufficient diversification as shown in eq. \@ref(eq:minvol). The sum of all weights equals one as given in eq. \@ref(eq:constraint1). The weights calculated are those which match the portfolio with a given $R_{p}$ as in eq. \@ref(eq:constraint2). This is a linear optimization problem as well as a formulation of the fundamental problem of balancing return and risk. Negative weightings are defined as short sales.

\begin{equation}
min \frac{1}{2}\vec{w}^\intercal\Sigma\vec{w}
\label{eq:minvol}
\end{equation}

*With constraints:*

*I.*
\begin{equation}
\vec{w}^\intercal\vec{1} = 1
\label{eq:constraint1}
\end{equation}

*II.*
\begin{equation}
\vec{w}^\intercal\vec{R} = R_{p}
\label{eq:constraint2}
\end{equation}

With the Lagrange multiplier, the function in eq. \@ref(eq:lagrange) is formed.

\begin{equation}
L(\vec{w}) = min \frac{1}{2}\vec{w}^\intercal\Sigma\vec{w} - \lambda(\vec{w}^\intercal\vec{R} - R_{p}) - \epsilon(\vec{w}^\intercal\vec{1} - 1)
\label{eq:lagrange}
\end{equation}

The gradient is set equal to zero as in eq. \@ref(eq:gradient). Together with the two constraints
\@ref(eq:constraint1) and \@ref(eq:constraint2), a linear system of equations with $n + 2$ equations and variables is formed.

\begin{equation}
\nabla_{w}L = \Sigma\vec{w} - \lambda*\vec{R} - \epsilon*\vec{1} = 0
\label{eq:gradient}
\end{equation}

<p> *$\vec{w}$ = asset weights* <p>
<p> *$\Sigma$ = covariance matrix* <p>
<p> *$\vec{1}$ = all-ones vector* <p>
<p> *$\vec{R}$ = mean returns* <p>
<p> *$R_{p}$ = portfolio return* <p>

\newpage

## Problems
In order to perform the optimization, some assumptions have to be made. As already mentioned before, return, volatility and correlation must be known. The volatility is described with the standard deviation and therefore no assumption about a normal distribution has to be made. Another condition applies to the linear relationship of the assets. Their correlation is not allowed to be either one or minus one. The determinant of such a correlation or covariance matrix would be zero and therefore it would not be invertible. To calculate the minimum variance and tangency portfolio, the covariance matrix $\Sigma$ must be invertible.

## Minimum variance portfolio
The minimum variance portfolio, short MVP, describes the portfolio with the lowest volatility. Since only the volatility is minimized, the condition *II* \@ref(eq:constraint2) is not required. Negative returns do not necessarily lead to negative weights. The calculation of the MVP does not depend directly on the returns, but they are reflected in the covariance matrix. The MVP is calculated as illustrated in eq. \@ref(eq:mvpw) and the corresponding weights are shown in table \@ref(tab:mvpwrtab).

\begin{equation}
\vec{w}_{mvp} = \frac{1}{\vec{1}^\intercal\Sigma^{-1}\vec{1}}*\Sigma^{-1}\vec{1}
\label{eq:mvpw}
\end{equation}

<p> *$\vec{w}_{mvp}$ = MVP weights* <p>
<p> *$\vec{1}$ = all-ones vector* <p>
<p> *$\Sigma$ = covariance matrix* <p>

```{r mvpwrtab}
mvpw_r = data.frame(colnames(r[-1]), mvp_weights(cov_mat(r)))
kable(mvpw_r, col.names = c("Stock", "Weight"), caption = "Minimum variance portfolio weights",
      digits = 3, booktabs = T, linesep = "")
```

\newpage

## Tangency portfolio 
The tangency portfolio, short TP, results from the capital market line and efficient frontier, which will be discussed in more detail in chapter \@ref(ef). The capital market line is an important component of the capital asset pricing model (CAPM). The slope of the capital market line indicates how much more return is expected per additional risk. The optimal ratio is found where the capital market line is tangential to the efficient frontier, hence the tangency portfolio. It is often referred to in the literature as market portfolio. [@sharpe]

The capital market line is shown in eq. \@ref(eq:cml) and the tangency portfolio in eq. \@ref(eq:tpw) with its corresponding weights in table \@ref(tab:tpwrtab). Note that in this paper the risk-free rate $R_f$ is already deducted from the returns directly as shown in eq. \@ref(eq:logr). Therefore, it can be set to zero in all further calculations.

*Capital market line:*
\begin{equation}
R_{p}(\sigma_{p}) = R_{f} + \frac{R_{tp} - R_{f}}{\sigma_{tp}} * \sigma_{p} = \frac{R_{tp}}{\sigma_{tp}} * \sigma_{p}
\label{eq:cml}
\end{equation}

<p> *$R_{p}$ = portfolio return as a function of $\sigma_{p}$* <p>
<p> *$\sigma_{p}$ = portfolio volatility* <p>
<p> *$R_{tp}$ = TP return* <p>
<p> *$\sigma_{tp}$ = TP volatility* <p>
<p> *$R_{f}$ =  risk-free rate of return* <p>

*Tangency portfolio:*
\begin{equation}
\vec{w}_{tp} = \frac{1}{\vec{1}^\intercal\Sigma^{-1}\vec{R}}*\Sigma^{-1}\vec{R}
\label{eq:tpw}
\end{equation}

<p> *$\vec{w}_{tp}$ = TP weights* <p>
<p> *$\vec{1}$ = all-ones vector* <p>
<p> *$\Sigma$ = covariance matrix* <p>
<p> *$\vec{R}$ = mean returns* <p>

```{r tpwrtab}
tpw_r = data.frame(colnames(r[-1]), tp_weights(cov_mat(r), mean_returns(r)))
kable(tpw_r, col.names = c("Stock", "Weight"), caption = "Tangency portfolio weights",
      digits = 3, booktabs = T, linesep = "")
```

\newpage

## Efficient Frontier {#ef}
The efficient frontier is a set of points that extends in the return-volatility diagram illustrated in figure \@ref(fig:refplot) between the MVP at the left edge of the reachable area and the TP. All portfolio above the MVP on this line are efficient because they have the highest possible return at a defined level of volatility.

\begin{equation}
\vec{w}_{ef} = \alpha * \vec{w}_{mvp} + (1 - \alpha) * \vec{w}_{tp}
\label{eq:efficientf}
\end{equation}

<p> *$\vec{w}_{ef}$ = efficient frontier weights* <p>
<p> *$\alpha$ = factor* <p>
<p> *$\vec{w}_{mvp}$ = MVP weights* <p>
<p> *$\vec{w}_{tp}$ = TP weights* <p>

\begin{equation}
R_{p} = \vec{w}^\intercal * \vec{R}
\label{eq:cord}
\end{equation}

<p> *$R_{p}$ = portfolio return* <p>
<p> *$\vec{w}$ = asset weights* <p>
<p> *$\vec{R}$ = mean returns* <p>

\begin{equation}
\sigma_{p} = \sqrt{\vec{w}^\intercal\Sigma\vec{w}}    
\label{eq:cord2}
\end{equation}

<p> *$\sigma_{p}$ = portfolio volatility* <p>
<p> *$\vec{w}$ = asset weights* <p>
<p> *$\Sigma$ = covariance matrix* <p>

```{r refplot, fig.cap = "Return-volatility diagram with efficient frontier of SMI constituents", fig.height = 5}
cm = cov_mat(r); mr = mean_returns(r)
efw_r = ef_weights(mvp_weights(cm), tp_weights(cm, mr), seq(-3, 3, 0.1))
efp_r = ef_points(efw_r, cm, mr)
efp = data.frame(n = stocks, vol = volatilities(cm), mr = mr, row.names = NULL)
suppressMessages(
  suppressWarnings(print(
    ggplot() +
      geom_abline(intercept = 0, slope = tp_point(efp_r)[2]/tp_point(efp_r)[1], alpha = 0.5) +
      geom_path(aes(x = efp_r[,1], y = efp_r[,2]), alpha = 0.5) +
      geom_point(aes(x = mvp_point(efp_r)[1], y = mvp_point(efp_r)[2], color = "MVP")) +
      geom_point(aes(x = tp_point(efp_r)[1], y = tp_point(efp_r)[2], color = "TP")) +
      geom_text_repel(data = efp, aes(x = vol, y = mr, label = n), size = 3) +
      lims(x = c(0, 3), y = c(-0.1, 0.2)) +
      labs(x = "Volatility [%]", y = "Expected return [%]", color = NULL) +
      scale_color_manual(values = c("MVP" = "cornflowerblue", "TP" = "orangered3")) +
      theme(legend.position = "bottom") +
      custom_theme_markdown)))
```

In figure \@ref(fig:refplot) is visible that the efficient frontier covers the range in which all possible portfolios are located. The TP achieves a significantly higher return than the stocks individually. The MVP is at the very left of the achievable area of the efficient frontier which means that there is no portfolio with a lower volatility.

\newpage

## Sharpe ratio {#sharpe}
The Sharpe ratio measures the performance of an asset, meaning its return compared to its risk. Generally, the greater the value of the Sharpe ratio, the more attractive the asset. In practice, the Sharpe ratio is not only positively received. In this paper however, the Sharpe ratio is relied on because it is a simple performance indicator. It is calculated by the return in excess of the risk-free rate per unit of volatility. As daily historical data is used in this paper, the Sharpe ratio is annualized by multiplication with $\sqrt{252}$. [@sharpe]

\begin{equation}
Sharpe ratio = \frac{R - R_{f}}{\sigma} = \frac{R}{\sigma}
\label{eq:sharperatio}
\end{equation}

<p> *$R$ = return* <p>
<p> *$R_{f}$ = risk-free rate of return* <p>
<p> *$\sigma_{i}$ = volatility of return* <p>

# Methodology {#methodology}

## Grouping {#grouping}
Grouping of assets is an approach to reduce the noise given by individual assets and therefore increase their performance. A qualitative and a mathematical procedure are applied.

### Grouping by industry
An intuitive approach is grouping stocks by industry. In the case of the SMI, stocks can be grouped as follows:

<p> **Consumer:** *Adecco, Nestle, Richemont, Swatch, Swisscom* <p>
<p> **Finance:** *Credit Suisse, Julius Baer, Swiss Life, Swiss Re, UBS, Zurich Insurance* <p>
<p> **Industrial:** *ABB, Geberit, Givaudan, LafargeHolcim, SGS* <p>
<p> **Pharma:** *Lonza, Novartis, Roche, Sika* <p>

To obtain the group returns, the mean value of the daily returns of all stocks within the group is calculated. Therefore, the group returns are a time series equaling the length of the historical data and all stocks within the group are weighted equally. From this time series, the mean returns, volatility and correlation can be calculated as shown in table \@ref(tab:grmrvoltab).

```{r grmrvoltab}
gr = groups_returns(returns, groups)
gr_mr_vol = data.frame(colnames(gr[-1]), mean_returns(gr), volatilities(cov_mat(gr)), row.names = NULL)
kable(gr_mr_vol, col.names = c("Group","Return [%]", "Volatility [%]"),
      caption = "Mean returns and volatility of groups by industry",
      digits = 3, booktabs = T, linesep = "")
```

\newpage

### Grouping with cluster analysis {#groupclust}
In a second approach grouping is based on the similarity measure which is calculated with Euclidean distances between two correlation coefficients as shown in eq. \@ref(eq:euclidean).

\begin{equation}
d_{i,j} = \sqrt{\vec{1}^\intercal(\vec{x}_{i} - \vec{x}_{j})^{2}}
\label{eq:euclidean}
\end{equation}

<p> *$d_{i,j}$ = Euclidean distance between asset i and asset j* <p>
<p> *$\vec{x}_{i}$ = correlation coefficients of asset i (column in cor. matrix)* <p>
<p> *$\vec{x}_{j}$ = correlation coefficients of asset j (column in cor. matrix)* <p>
<p> *$\vec{1}$ = all-ones vector* <p>

When eq. \@ref(eq:euclidean) is applied to the correlation matrix shown in figure \@ref(fig:corplot), it results in a distance matrix with the same dimensions. The algorithm sorts the values of the distance matrix with the complete-linkage method.

**Step 1**
Group the two assets with the smallest distance value.

**Step 2**
Compare the created group with individual assets or other created groups. The distance between a group ${i,j}$ and an individual asset $k$ is the largest possible distance between an asset within the group and the individual asset.\newline
$d\langle \{i,j\}, k \rangle = max(d\langle i, k \rangle, d\langle j, k \rangle)$.\newline
For distances between groups, the largest possible distance is taken from the distances between each asset of a group with an asset of another group. For distance between group ${i,j}$ and ${k,l}$ with two assets applies:\newline
$d\langle \{i,j\}, \{k,l\} \rangle = max(d\langle i, k \rangle, d\langle i, l \rangle, d\langle j, k \rangle, d\langle j, l \rangle)$.

**Step 3**
Repeat the previous steps with the newly created distance matrix until only two groups remain. The outcome is a dendrogram which is also part of the illustration in figure \@ref(fig:distheatmap). The first step of grouping is the connection which is drawn lowest. The color scaling indicates the different distances. [@stdm]

```{r distheatmap, fig.cap = "Stocks grouped by Euclidean distance of correlation", fig.height = 7}
cor = cor_mat(returns)
heatmap3(as.matrix(dist(cor)), method = "complete", symm = T, margins = c(7, 7),
         col = colorRampPalette(c("orangered4", "orangered2", "honeydew"))(1000))
```

\newpage

<p> **Group 1:** *ABB, Adecco, Credit Suisse, Julius Baer, LafargeHolcim, Richemont, Swatch, Swiss Life, Swiss Re, UBS, Zurich Insurance* <p>
<p> **Group 2:** *Givaudan, Lonza, Nestle, Novartis, Roche, Swisscom* <p>
<p> **Group 3:** *Geberit, SGS, Sika* <p>

Using the algorithm described, the 20 stocks can now be divided into three groups. The dendrogram in figure \@ref(fig:distheatmap) shows the allocation. This division makes mathematical sense but is hardly ever used in practice. First, the groups are different in terms of size. *Group 1* consists of eleven stocks but *Group 3* of only three. This means that with equal weights within the group, the stocks in *Group 3* are weighted higher overall than those in *Group 1*. Second, one would have to explain to an investor why stocks from different industries are grouped together. For example, *ABB* is grouped with *UBS* but not with *Geberit* which is intuitively contradictory.

Figure \@ref(fig:gefplot) gives some visual intuition. The groups contain stocks with similar volatility and return, so it makes sense that *ABB* is grouped with *UBS* because and not with *Geberit*.

```{r gefplot, fig.cap = " Return-volatility diagram with efficient frontier of groups by correlation", fig.height = 5}
g2r = groups_returns(r, groups2)
g2cm = cov_mat(g2r); g2mr = mean_returns(g2r)
efw_g2r = ef_weights(mvp_weights(g2cm), tp_weights(g2cm, g2mr), seq(-3, 3, 0.1))
efp_g2r = ef_points(efw_g2r, g2cm, g2mr)
efp = data.frame(n = c(colnames(r[-1]), colnames(g2r[-1])),
                 vol = c(volatilities(cm), volatilities(g2cm)), mr = c(mr, g2mr), row.names = NULL)
g = c()
for (i in efp$n[1:length(colnames(r[-1]))]) {
  g = c(g, ifelse(identical(
    names(which(setNames(unlist(groups2, use.names = F),
                         rep(names(groups2), lengths(groups2))) == i)), character(0)),
    "Excluded", names(which(setNames(unlist(groups2, use.names = F),
                                     rep(names(groups2), lengths(groups2))) == i))))
}
efp$g = c(g, colnames(g2r[-1]))
efp_sub = efp[21:23,]
suppressMessages(
  suppressWarnings(print(
    ggplot() +
      geom_abline(intercept = 0, slope = tp_point(efp_g2r)[2]/tp_point(efp_g2r)[1], alpha = 0.5) +
      geom_path(aes(x = efp_g2r[,1], y = efp_g2r[,2]), alpha = 0.5) +
      geom_point(aes(x = mvp_point(efp_g2r)[1], y = mvp_point(efp_g2r)[2]), color = "cornflowerblue") +
      geom_point(aes(x = tp_point(efp_g2r)[1], y = tp_point(efp_g2r)[2]), color = "orangered3") +
      geom_point(data = efp_sub, aes(x = vol, y = mr, color = g), size = 1) +
      geom_text_repel(data = efp, aes(x = vol, y = mr, label = n, color = g), size = 3,
                      key_glyph = draw_key_point) +
      lims(x = c(0, 3), y = c(-0.1, 0.2)) +
      labs(x = "Volatility [%]", y = "Expected return [%]", color = NULL) +
      scale_color_manual(values = c("Group 1" = "forestgreen", "Group 2" = "tan1",
                                    "Group 3" = "darkgray")) +
      theme(legend.position = "bottom") +
      custom_theme_markdown)))
```

Table \@ref(tab:g2rmrvoltab) shows return and volatility of the three groups. It is visible that the values have improved overall compared to table \@ref(tab:grmrvoltab). Further cluster analysis methods such as k-means or k-medoids could be considered but are not discussed in this paper.

```{r g2rmrvoltab}
gr_mr_vol = data.frame(colnames(g2r[-1]), mean_returns(g2r), volatilities(cov_mat(g2r)),
                       row.names = NULL)
kable(gr_mr_vol, col.names = c("Group","Return [%]", "Volatility [%]"),
      caption = "Mean returns and volatility of groups by correlation",
      digits = 3, booktabs = T, linesep = "")
```

\newpage

## Bootstrap {#bootstrap}
Bootstrapping is a method for resampling. It creates a new sample by drawing from an existing sample. With 100 or more samples, statistical measures such as mean and variance can be calculated across all samples. This is useful to draw conclusions about a population from just one sample.

In this case, bootstrapping is applied to analyze the standard deviation of the MVP and TP weights by resampling all historical returns.

**Step 1**
The historical returns have the structure of a matrix with the rows being each day and the columns each stock. This matrix is sampled row-wise using `sample()` with `replace = T`. In other words, one row, which corresponds to one day, is drawn randomly from the initial matrix as many times as the initial matrix has rows. As `replace = F`, one row/day can be drawn multiple times. If `replace = F`, the correlation matrix of the sample would be equal to the one of the initial matrix which is not desired. The result of this step is a sample returns matrix equaling the dimensions of the initial historical returns matrix.

**Step 2**
From the sample returns matrix, the mean return and covariance matrix are calculated to obtain the MVP and TP weights.

**Step 3**
Repeat step one and two 100 times to obtain 100 sets of MVP and TP weights. The standard deviation across these 100 sets of weights is calculated per stock. It is computed with the 84-quantile minus the 50-quantile to achieve a more robust result.

In figure \@ref(fig:bsef) all bootstrap samples are visualized with their MVP and TP as well as efficient frontier. Noticeable is the variance of the TP compared to the MVP. This is due to the high standard errors of the returns. A more detailed explanation is given in the results \@ref(res1).

```{r bsef, fig.cap = "Bootstrap samples efficiency frontier", fig.height = 4}
bs_r = bootstrap(r); bs_gr = bootstrap(gr)
suppressMessages(suppressWarnings(print(gg_bootstrap_ef(bs_r$samples_ef_points,
                                                        theme = custom_theme_markdown))))
```

\newpage

## Cross validation {#crossvalidation}
Cross validation is a model validation technique for assessing how a model will generalize to an independent data set, i.e., in another time period. The data set is split into a training set used to train the model and a test set to evaluate its performance. This procedure is replicated multiple times until all data was once in the test set.

In this case, the historical data is split into five sections and consequentially five models are trained as depicted in figure \@ref(fig:crossvalidation). The gray sections are the training sets and the red ones the test sets.

```{r crossvalidation, fig.cap = "Cross validation training and test sets", out.width="95%"}
knitr::include_graphics("./Images/Cross_validation.jpg")
```

In the case of the Markovitz model, the mean returns and covariance matrix of the training set are used to calculate the asset weights. These weights are then applied to the test set asset returns according to eq. \@ref(eq:weightedreturn).

\begin{equation}
R_{i,w} = \vec{w}^\intercal * \vec{R_i}
\label{eq:weightedreturn}
\end{equation}

<p> *$R_{i,w}$ = weighted return day i* <p>
<p> *$\vec{w}$ = asset weights* <p>
<p> *$\vec{R_i}$ = asset returns day i* <p>

At the end of the cross validation, all five test sets are combined into one time series consisting daily weighted returns and equaling the length of the historical data. This time series is entirely out of sample and is used to calculate the Sharpe ratio and hence measure the performance of the model. The implicit assumption behind this procedure is that the returns at different points in time are independent random variables (and, in particular, do not “remember” the past).

It is to note that the training and test sets of the five sections vary significantly in some cases. To give one cause for this, the financial crisis from 2007–2008 can be looked at. During this period, most assets yielded negative returns. For example, model two in figure \@ref(fig:crossvalidation) includes this time period as part of the test set and model four of the training set. This results in fluctuating asset weights across the five models which are illustrated in figure \@ref(fig:tpwbymodel).

\newpage

```{r tpwbymodel, fig.cap = "Tangency portfolio weights by model", fig.height = 5}
is_r = in_sample(r); is_gr = in_sample(gr); is_g2r = in_sample(g2r)
sets_r = cross_validation_sets(r); sets_gr = cross_validation_sets(gr)
sets_g2r = cross_validation_sets(g2r)
os_r = out_of_sample(sets_r); os_gr = out_of_sample(sets_gr); os_g2r = out_of_sample(sets_g2r)
sets_tpw = matrix(ncol = length(sets_r[[1]]), nrow = ncol(r[-1]))
for (i in 1:5) {sets_tpw[,i] = out_of_sample(sets_r, set = i)$tp_weights}
sets_tpw = data.frame(stocks = stocks, sets_tpw)
alpha = 0.2
ggplot(sets_tpw) +
  geom_line(aes(x = stocks, y = X1, group = 1, col = "Model 1"), alpha = alpha) +
  geom_point(aes(x = stocks, y = X1, group = 1, col = "Model 1")) +
  geom_line(aes(x = stocks, y = X2, group = 2, col = "Model 2"), alpha = alpha) +
  geom_point(aes(x = stocks, y = X2, group = 2, col = "Model 2")) +
  geom_line(aes(x = stocks, y = X3, group = 3, col = "Model 3"), alpha = alpha) +
  geom_point(aes(x = stocks, y = X3, group = 3, col = "Model 3")) +
  geom_line(aes(x = stocks, y = X4, group = 4, col = "Model 4"), alpha = alpha) +
  geom_point(aes(x = stocks, y = X4, group = 4, col = "Model 4")) +
  geom_line(aes(x = stocks, y = X5, group = 5, col = "Model 5"), alpha = alpha) +
  geom_point(aes(x = stocks, y = X5, group = 5, col = "Model 5")) +
  labs(x = NULL, y = "Weight", color = NULL) +
  scale_color_manual(values = c("Model 1" = "orangered3", "Model 2" = "cornflowerblue",
                                "Model 3" = "forestgreen", "Model 4" = "tan1",
                                "Model 5" = "darkgray")) +
  guides(fill = guide_legend(ncol = 5)) +
  theme(axis.text.x = element_text(angle = 90), legend.position = "bottom") +
  custom_theme_markdown
```

## Shrinkage {#shrinkage}
Shrinkage is an approach with the objective to increase the out of sample Sharpe ratio by altering the inputs of the Markovitz optimization, namely the mean returns and covariance matrix.

### Shrinkage of mean returns
The shrunk mean returns are calculated as in eq. \@ref(eq:shrinkr).

\begin{equation}
\vec{R}(\lambda) = \lambda*\vec{R} + (1 - \lambda) * \vec{\overline{R}}
\label{eq:shrinkr}
\end{equation}

<p> *$\vec{R}(\lambda)$ = mean returns as a function of $\lambda$* <p>
<p> *$\lambda$ = shrinkage factor* <p>
<p> *$\vec{R}$ = mean returns* <p>
<p> *$\vec{\overline{R}}$ = mean of mean returns* <p>

When the shrinkage factor $\lambda$ is set to one, the mean returns remain unchanged. When it is decreased from one towards zero, the mean returns converge towards their mean value. Once the shrinkage factor $\lambda$ is zero and therefore all mean returns are equal, the tangency portfolio eq. \@ref(eq:tpw) obtains the same asset weights as the minimum variance portfolio eq. \@ref(eq:mvpw).

There is an important constraint in mean returns shrinkage. Take the scaling factor of the tangency portfolio eq. \@ref(eq:tpw) and replace $\vec{R}$ with eq. \@ref(eq:shrinkr) as shown in eq. \@ref(eq:tpwsf).

\begin{equation}
\vec{w}_{tp,sf} = \frac{1}{\vec{1}^\intercal\Sigma^{-1}(\lambda*\vec{R} + (1 - \lambda) * \vec{\overline{R}})}
\label{eq:tpwsf}
\end{equation}

It is clear that for some value of $\lambda$ the denominator becomes zero. Computationally, it is already problematic if the denominator is close to zero. When setting the denominator equal to zero, the value of $\lambda$ at the zero crossing can be calculated as in eq. \@ref(eq:lambdazero).

\begin{equation}
\lambda_0 = \frac{-\vec{1}^\intercal\Sigma^{-1}\vec{\overline{R}}}{\vec{1}^\intercal\Sigma^{-1}\vec{R} - \vec{1}^\intercal\Sigma^{-1}\vec{\overline{R}}}
\label{eq:lambdazero}
\end{equation}

\newpage

If the zero crossing is in the shrinkage interval of $[0, 1]$, this results in extreme asset weights. As the denominator approaches zero, the scaling factor in eq. \@ref(eq:tpwsf) becomes very large which affects the weights. To illustrate this effect, the weights of the five cross validation models from figure \@ref(fig:crossvalidation) are plotted as a function of $\lambda$ in figure \@ref(fig:tpwshrinkbymodel).

```{r tpwshrinkbymodel, fig.cap = "Tangency portfolio weights by model as a function of $\\lambda$", fig.height = 5}
n = ncol(r[-1]); start = 0; stop = 1; size = 0.2
s1 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 1)
out1 = t(sapply(s1, function(x){x$tp_weights}))
s2 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 2)
out2 = t(sapply(s2, function(x){x$tp_weights}))
s3 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 3)
out3 = t(sapply(s3, function(x){x$tp_weights}))
s4 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 4)
out4 = t(sapply(s4, function(x){x$tp_weights}))
s5 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 5)
out5 = t(sapply(s5, function(x){x$tp_weights}))
cm3 = solve(sets_r[[1]][[3]]$cov_mat); mr3 = sets_r[[1]][[3]]$mean_returns
denom_zero = (-rowSums(cm3) %*% rep(mean(mr3), n)) / (rowSums(cm3) %*% mr3 - rowSums(cm3) %*% rep(mean(mr3), n))
gg1 = gg2 = gg3 = gg4 = gg5 = ggplot() +
  scale_x_continuous(breaks = c(0, 0.5, 1)) + custom_theme_markdown
for (i in 1:20) {gg1 = gg1 + geom_line(aes_string(x = seq(start, stop, 0.01), y = out1[,i]),
                                       size = size)}
gg1 = gg1 + labs(x = expression(lambda), y = "Weight", title = "Model 1")
for (i in 1:20) {gg2 = gg2 + geom_line(aes_string(x = seq(start, stop, 0.01), y = out2[,i]),
                                       size = size)}
gg2 = gg2 + labs(x = expression(lambda), y = "Weight", title = "Model 2")
for (i in 1:20) {gg3 = gg3 + geom_line(aes_string(x = seq(start, stop, 0.01), y = out3[,i]),
                                       size = size)}
gg3 = gg3 + geom_vline(xintercept = denom_zero, color = "orangered3") +
  labs(x = expression(lambda), y = "Weight", title = "Model 3")
for (i in 1:20) {gg4 = gg4 + geom_line(aes_string(x = seq(start, stop, 0.01), y = out4[,i]),
                                       size = size)}
gg4 = gg4 + labs(x = expression(lambda), y = "Weight", title = "Model 4")
for (i in 1:20) {gg5 = gg5 + geom_line(aes_string(x = seq(start, stop, 0.01), y = out5[,i]),
                                       size = size)}
gg5 = gg5 + labs(x = expression(lambda), y = "Weight", title = "Model 5")
ggarrange(gg1, gg2, gg3, gg4, gg5)
```

Model three in figure \@ref(fig:tpwshrinkbymodel) shows such extreme asset weights ranging from approximately -20 to 20. The asymptote is visualized as a vertical red line. It is not optimal when such extreme weights are applied to returns as done in the cross validation. This can negatively influence further calculations and lead to undesired results. The other four models do show asset weights in a normal scale.

The scaling factor as a function of $\lambda$ is shown in figure \@ref(fig:tpwsfshrinkbymodel).

```{r tpwsfshrinkbymodel, fig.cap = "Tangency portfolio weights scaling factor by model as a function of $\\lambda$", fig.height = 5}
s1 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 1)
out1 = t(sapply(s1, function(x){1 / (rep(1, n) %*% solve(x$shrinking_cov_mat) %*% x$shrinking_mean_returns)}))
s2 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 2)
out2 = t(sapply(s2, function(x){1 / (rep(1, n) %*% solve(x$shrinking_cov_mat) %*% x$shrinking_mean_returns)}))
s3 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 3)
out3 = t(sapply(s3, function(x){1 / (rep(1, n) %*% solve(x$shrinking_cov_mat) %*% x$shrinking_mean_returns)}))
s4 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 4)
out4 = t(sapply(s4, function(x){1 / (rep(1, n) %*% solve(x$shrinking_cov_mat) %*% x$shrinking_mean_returns)}))
s5 = out_of_sample_vec(sets_r, seq(start, stop, 0.01), set = 5)
out5 = t(sapply(s5, function(x){1 / (rep(1, n) %*% solve(x$shrinking_cov_mat) %*% x$shrinking_mean_returns)}))
gg1 = gg2 = gg3 = gg4 = gg5 = ggplot() + expand_limits(y = 0) +
  scale_x_continuous(breaks = c(0, 0.5, 1)) +  custom_theme_markdown
gg1 = gg1 + geom_line(aes(x = seq(start, stop, 0.01), y = out1[1,]), size = size) +
  labs(x = expression(lambda), y = "Scaling factor", title = "Model 1")
gg2 = gg2 + geom_line(aes(x = seq(start, stop, 0.01), y = out2[1,]), size = size) +
  labs(x = expression(lambda), y = "Scaling factor", title = "Model 2")
gg3 = gg3 + geom_line(aes(x = seq(start, stop, 0.01), y = out3[1,]), size = size) + geom_vline(xintercept = denom_zero, color = "orangered3") +
  labs(x = expression(lambda), y = "Scaling factor", title = "Model 3")
gg4 = gg4 + geom_line(aes(x = seq(start, stop, 0.01), y = out4[1,]), size = size) +
  labs(x = expression(lambda), y = "Scaling factor", title = "Model 4")
gg5 = gg5 + geom_line(aes(x = seq(start, stop, 0.01), y = out5[1,]), size = size) +
  labs(x = expression(lambda), y = "Scaling factor", title = "Model 5")
ggarrange(gg1, gg2, gg3, gg4, gg5)
```

The zero crossing of the scaling factor in model three can be seen clearly. When working with mean returns shrinkage, one has to be aware of this behavior.

\newpage

### Shrinkage of correlation matrix
The shrunk correlation matrix is calculated as in eq. \@ref(eq:shrinkcor).

\begin{equation}
\rho_{i,j}(\epsilon) = I_{i,j} + \epsilon * \tilde{\rho}_{i,j}
\label{eq:shrinkcor}
\end{equation}

<p> *$\rho_{i,j}(\epsilon)$ = correlation matrix at i,j as a function of $\epsilon$* <p>
<p> *$\epsilon$ = shrinkage factor* <p>
<p> *$I_{i,j}$ = identity matrix at i,j* <p>
<p> *$\tilde{\rho}_{i,j}$ = correlation matrix with diagonal zero at i,j* <p>

When the shrinkage factor $\epsilon$ is set to one, the correlation matrix remains unchanged. When it is decreased from one towards zero, the correlation coefficients converge towards zero, except for the diagonal which always remains one. As the tangency portfolio eq. \@ref(eq:tpw) requires the covariance matrix, it is first standardized to the correlation matrix according to eq. \@ref(eq:cor), then shrunk with eq. \@ref(eq:shrinkcor) and scaled up to the covariance matrix again according to eq. \@ref(eq:cor).

There can also arise the case, where for some shrinkage factor $\epsilon$ the denominator of the tangency portfolio eq. \@ref(eq:tpw) becomes zero or close to zero. However, this is not further elaborated on.

# Results
The results of the different approaches described in chapter \@ref(methodology) are presented and compared by their Sharpe ratio.

## SMI constituents {#res1}
Before the results of the optimization for all individual stocks are shown, the standard deviation of the bootstrapped MVP and TP weights are compared. Figure \@ref(fig:mvpse) illustrates the mean weights and their standard deviation of the MVP and figure \@ref(fig:tpse) of the TP.

```{r mvpse, fig.cap = "MVP weights with standard deviation", fig.height = 4}
mvpw_r = mvp_weights(cov_mat(r)); mvpw_gr = mvp_weights(cov_mat(gr))
mvpw_sd_r = bs_r$mvp_weights_sd; mvpw_sd_gr = bs_gr$mvp_weights_sd
minmvp = min(c(mvpw_r - mvpw_sd_r, mvpw_gr - mvpw_sd_gr))
maxmvp = max(c(mvpw_r + mvpw_sd_r, mvpw_gr + mvpw_sd_gr))
gg_errorbar(stocks, mvpw_r, mvpw_sd_r, c(minmvp, maxmvp), "Weights", theme = custom_theme_markdown)
```

\newpage

```{r tpse, fig.cap = "TP weights with standard deviation", fig.height = 4}
tpw_r = tp_weights(cov_mat(r), mean_returns(r)); tpw_gr = tp_weights(cov_mat(gr), mean_returns(gr))
tpw_sd_r = bs_r$tp_weights_sd; tpw_sd_gr = bs_gr$tp_weights_sd
mintp = min(c(tpw_r - tpw_sd_r, tpw_gr - tpw_sd_gr))
maxtp = max(c(tpw_r + tpw_sd_r, tpw_gr + tpw_sd_gr))
gg_errorbar(stocks, tpw_r, tpw_sd_r, c(mintp, maxtp), "Weights", theme = custom_theme_markdown)
```

It is seen clearly in figure \@ref(fig:mvpse) and \@ref(fig:tpse) and also visualized before in figure \@ref(fig:bsef) in chapter \@ref(bootstrap), that TP weights have a higher standard deviation than MVP weights. When considering the MVP eq. \@ref(eq:mvpw) and TP eq. \@ref(eq:tpw), it is evident that latter directly depends on the returns which have high standard errors. This carries over to the standard deviation of the TP weights. Although the returns also impact the covariance, it only varies to a small degree which is seen in the small standard deviation of the MVP.

Table \@ref(tab:srcomtab1) compares the in and out of sample Sharpe ratio for all 20 stocks.

```{r srcomtab1}
srcomtab1 = data.frame(is_r, os_r, row.names = "SMI constituents")
kable(srcomtab1, col.names = c("In sample", "Out of sample"),
      caption = "Sharpe ratio of SMI constituents",
      digits = 3, booktabs = T, linesep = "")
```

In sample, training and test set are equal and therefore its Sharpe ratio is higher than out of sample. This is because the model performs best on the data which built it. Out of sample, training set and test set differ. If a stock behaves differently in a test set compared to the training set used to build the model, this stock can easily be over- or undervalued. This leads to a worse performance of the model and therefore lower Sharpe ratio. The conclusion that can be drawn from this insight is, that the in sample Sharpe ratio should always be higher than the out of sample Sharpe ratio.

\newpage

## Groups by industry and correlation {#res2}
Figure \@ref(fig:mvpsegr) illustrates the mean weights and their standard deviation of the MVP and figure \@ref(fig:tpsegr) of the TP.

```{r mvpsegr, fig.cap = "MVP weights with standard deviation by group", fig.height = 3}
gg_errorbar(colnames(gr[-1]), mvpw_gr, mvpw_sd_gr, c(minmvp, maxmvp), "Weights",
            theme = custom_theme_markdown)
```

```{r tpsegr, fig.cap = "TP weights with standard deviation by group", fig.height = 3}
gg_errorbar(colnames(gr[-1]), tpw_gr, tpw_sd_gr, c(mintp, maxtp), "Weights",
            theme = custom_theme_markdown)
```

The same conclusion can be drawn for the grouped stocks as for the individual stocks in chapter \@ref(res1). The TP weights standard deviation is also much larger than the one of the MVP.

In table \@ref(tab:srcomtab2), the in and out of sample Sharpe ratios for both groups is shown.

```{r srcomtab2}
srcomtab2 = data.frame(c(is_gr, is_g2r),
                 c(os_gr, os_g2r), row.names = c("Groups by industry", "Groups by correlation"))
kable(srcomtab2, col.names = c("In sample", "Out of sample"),
      caption = "Sharpe ratio of groups", 
      digits = 3, booktabs = T, linesep = "")
```

The first observation is, that the in sample Sharp ratio decreased after the grouping is applied compared to the individual stocks in table \@ref(tab:srcomtab1). The reason being, that with 20 parameters a better fit to the data is achieved than with three or four parameters as it is the case for the groups. Nevertheless, the out of sample Sharpe ratio has increased. Due to grouping, the returns are less noisy than the ones of the individual stocks. This leads to a more reliable model which performs better out of sample.

In chapter \@ref(groupclust), cluster analysis is used to optimize the group composition. Hence, higher correlating stocks are grouped together to obtain a better diversified portfolio than is achieved with grouping by industry. This leads to a higher in and out of sample Sharpe ratio.

\newpage

## Optimal shrinkage factors {#res3}

### Shrinkage factors analyzed separately
The Sharpe ratio can be visualized as a function of the shrinkage factor. Figure \@ref(fig:srplot) shows such a plot for the mean returns shrinkage factor.

When looking at the SMI constituents curve, it is seen that the highest Sharpe ratio is achieved with a shrinkage factor of 0.05. If past returns are taken too seriously (higher shrinkage factor) or not considered at all (shrinkage factor zero), the out of sample Sharpe ratio drops. So, the returns can be trusted only five percent. This result is plausible, as the returns are very noisy. Further can be seen that the Sharpe ratio has a sharp bend and drop below zero. This is caused by the close to zero denominator of the tangency portfolio eq. \@ref(eq:tpw) as described in chapter \@ref(shrinkage).

For the two group curves the result is inverted. As the grouping of the stocks already reduces the noise significantly, shrinkage does show almost no improvement over unshrunk mean returns. The groups by correlation do show a significantly higher Sharpe ratio than the groups by industry and it is also higher than the one of the SMI constituents.

```{r srplot, fig.cap = "Sharpe ratio as a function of return shrinking factor", fig.height = 4}
os_r_sr = unlist(out_of_sample_vec(sets_r, seq(0, 1, 0.01)))
os_gr_sr = unlist(out_of_sample_vec(sets_gr, seq(0, 1, 0.01)))
os_g2r_sr = unlist(out_of_sample_vec(sets_g2r, seq(0, 1, 0.01)))
gg_shrink2D(list(os_r_sr, os_gr_sr, os_g2r_sr),
            c("SMI constituents", "Groups by industry", "Groups by correlation"), "Return",
            theme = custom_theme_markdown)
```

Figure \@ref(fig:scorplot) shows the plot for the correlation shrinkage factor. For the SMI constituents, the same observation can be made as in figure \@ref(fig:srplot). The out of sample Sharpe ratio drops, if past correlations are taken too seriously (higher shrinkage factor) or less seriously (shrinkage factor zero). The optimum is found at 0.4. Also, the result for groups is similar to the previous one in the sense that shrinkage does not have a great impact. The highest Sharpe ratio is again achieved by the groups by correlation.

```{r scorplot, fig.cap = "Sharpe ratio as a function of correlation shrinking factor", fig.height = 4}
os_r_scor = unlist(out_of_sample_vec(sets_r, 1, seq(0, 1, 0.01)))
os_gr_scor = unlist(out_of_sample_vec(sets_gr, 1, seq(0, 1, 0.01)))
os_g2r_scor = unlist(out_of_sample_vec(sets_g2r, 1, seq(0, 1, 0.01)))
gg_shrink2D(list(os_r_scor, os_gr_scor, os_g2r_scor),
            c("SMI constituents", "Groups by industry", "Groups by correlation"), "Correlation",
            theme = custom_theme_markdown)
```

\newpage

### Shrinkage factors analyzed simultaneously
A further visualization is a heatmap plot where the Sharpe ratio is plotted as a function of both shrinkage factors. The darker the area, the higher the Sharpe ratio.

Figure \@ref(fig:srcorplot1) shows such a plot for the SMI constituents. In the bottom part and left part of the plot the contour lines are very chaotic which is caused by the Sharpe ratio dropping quickly and below zero. The highest Sharpe ratio is achieved with a mean returns shrinkage factor of 0.05 and a correlation shrinkage factor of one. This matches the result of the figure \@ref(fig:srplot) and \@ref(fig:scorplot) where mean return shrinkage is better at improving the Sharpe ratio than correlation shrinkage, which does not matter in this case.

```{r srcorplot1, fig.cap = "SMI constituents Sharpe ratio as a function of return and correlation shrinkage factor", fig.height = 4}
grid = expand.grid(seq(0, 1, by = 0.05), seq(0, 1, by = 0.05))
os_r_sr_scor = unlist(out_of_sample_vec(sets_r, grid[,1], grid[,2]))
gg_shrink3D(grid, os_r_sr_scor, theme = custom_theme_markdown)
```

Figure \@ref(fig:srcorplot2) shows the plot for the groups by industry and figure \@ref(fig:srcorplot3) for the groups by correlation. Both plots are very similar in their general structure. The groups by industry also show a sudden decrease in Sharpe ratio in the lower portion while the groups by correlation do not. Overall, latter does show a Sharpe ratio which is 0.38 higher than the one of the groups by industry.

```{r srcorplot2, fig.cap = "Groups by industry Sharpe ratio as a function of return and correlation shrinkage factor", fig.height = 4}
os_gr_sr_scor = unlist(out_of_sample_vec(sets_gr, grid[,1], grid[,2]))
gg_shrink3D(grid, os_gr_sr_scor, theme = custom_theme_markdown)
```

\newpage

```{r srcorplot3, fig.cap = "Groups by correlation Sharpe ratio as a function of return and correlation shrinkage factor", fig.height = 4}
os_g2r_sr_scor = unlist(out_of_sample_vec(sets_g2r, grid[,1], grid[,2]))
gg_shrink3D(grid, os_g2r_sr_scor, theme = custom_theme_markdown)
```

Table \@ref(tab:srcomtab3) sums up all investigated Sharpe ratios, namely in sample (IS), out of sample (OS) and out of sample with applied shrinkage (OS shrinkage).

```{r srcomtab3}
srcomtab3 = data.frame(c(is_r, is_gr, is_g2r),
                 c(os_r, os_gr, os_g2r),
                 c(max(os_r_sr_scor), max(os_gr_sr_scor), max(os_g2r_sr_scor)),
                 row.names = c("SMI constituents", "Groups by industry", "Groups by correlation"))
kable(srcomtab3, col.names = c("IS", "OS", "OS shrinkage"),
      caption = "Sharpe ratio overview", 
      digits = 3, booktabs = T, linesep = "")
```

To give further insights, the TP weights for unshrunk and shrunk mean returns and correlations are compared in tables \@ref(tab:tpwrstab), \@ref(tab:tpwgrtab) and \@ref(tab:tpwg2rtab).

The weights of unshrunk mean returns and correlations are the ones previously shown in table \@ref(tab:tpwrtab) in chapter \@ref(markovitz). They are also the ones which obtain the in sample Sharpe ratio, where all historical data is used to train the model. The shrinkage factors utilized for the shrunk mean returns and correlations are the ones which obtained the highest out of sample Sharpe ratio in figure \@ref(fig:srcorplot1), \@ref(fig:srcorplot2) and \@ref(fig:srcorplot3).

```{r tpwrstab}
mr = mean_returns(r)
smr = shrinkage_mr(mr, 0.05)
tpw_r = data.frame(colnames(r[-1]), tp_weights(cov_mat(r), mr), tp_weights(cov_mat(r), smr))
kable(tpw_r, col.names = c("Stock", "Before shrinkage", "After shrinkage"),
      caption = "Tangency portfolio weights",
      digits = 3, booktabs = T, linesep = "")
```

\newpage

When comparing the two sets of weights, a significant difference can be seen. Overall, the weights after shrinkage are closer around zero as they are before. This is caused by the fact that the returns are taken into account only five percent. Therefore, the weights are much closer to the MVP weights as seen in table \@ref(tab:mvpwrtab) in chapter \@ref(markovitz).

Compared to the weights of the individual stocks already shown in table \@ref(tab:tpwrtab), the weights of the groups in table \@ref(tab:srcomtab2) and \@ref(tab:srcomtab3) are significantly higher or lower. To recall, the weights must sum up to one according to constraint \@ref(eq:constraint1). If the groups perform differently, like it is the case for *Finance* (negative mean return) and *Pharma* (positive mean return) in the groups by industry, the weights drift apart. This becomes even more prominent in the groups by correlation between *Group 1* and *Group 3*. As mentioned in chapter \@ref(markovitz), negative weights correspond to short sales. The difference between the weights before and after shrinkage is insignificant. This is attributed to the shrinkage factor of the mean returns being one and the one for the correlation being 0.95. So there is almost no shrinkage applied.

```{r tpwgrtab}
gmr = mean_returns(gr)
gscov = shrinkage_cor(cov_mat(gr), 0.95)
tpw_gr_df = data.frame(colnames(gr[-1]), tp_weights(cov_mat(gr), gmr), tp_weights(gscov, gmr))
kable(tpw_gr_df, col.names = c("Group", "Before shrinkage", "After shrinkage"),
      caption = "Groups by industry in sample tangency portfolio weights",
      digits = 3, booktabs = T, linesep = "")
```

```{r tpwg2rtab}
g2mr = mean_returns(g2r)
g2scov = shrinkage_cor(cov_mat(g2r), 0.95)
tpw_g2r_df = data.frame(colnames(g2r[-1]), tp_weights(cov_mat(g2r), g2mr), tp_weights(g2scov, g2mr))
kable(tpw_g2r_df, col.names = c("Group", "Before shrinkage", "After shrinkage"),
      caption = "Groups by correlation in sample tangency portfolio weights",
      digits = 3, booktabs = T, linesep = "")
```

\newpage

# Conclusion

Considered in isolation, the highest increase of Sharpe ratio is achieved by grouping by correlation using cluster analysis. Nevertheless the highest Sharpe ratio is obtained with a combination of grouping and shrinkage. It should be noted that when one method is applied after the other, its influence on the Sharpe ratio is significantly lower. This has to do with the fact that the noise can only be reduced to a certain degree. In this case, the first method grouping, has a larger increase in Sharpe ratio, while shrinkage, as the second method, has only a small additional increase. However, it is well apparent that for the SMI constituents where only shrinkage is applied, the rise in Sharpe ratio is similarly high compared with grouping as the first method. 

With these insights, the best procedure for optimizing multiple assets can be derived. In a first step, one would group by correlation to reduce noise by a significant amount. The second step would be applying shrinkage to mean returns and correlation. Examining the groups implies that returns remain unshrunk, while correlation is shrunk by a factor of 0.95. While this is not generally valid, overall it can be said that shrinkage offers only a small gain in addition to the previous grouping. But if grouping is omitted, it is possible that a higher Sharpe ratio can be achieved with the shrinkage approach, especially with a high number of assets. 

\newpage

# References
<div id="refs"></div>

\listoftables

\newpage

\listoffigures
